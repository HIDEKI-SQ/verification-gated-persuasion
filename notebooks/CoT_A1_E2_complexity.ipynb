{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT A1-E2: Task Complexity Gradient\n",
    "\n",
    "## Purpose\n",
    "Test whether **task complexity** (number of reasoning steps) affects CIF vulnerability.\n",
    "\n",
    "## Hypothesis\n",
    "- Simple tasks (1-2 steps): Low CIF - model can verify easily\n",
    "- Complex tasks (4-5 steps): High CIF - model relies more on external reasoning\n",
    "\n",
    "## Design\n",
    "| Complexity | Steps | Example |\n",
    "|------------|-------|----------|\n",
    "| Simple | 1-2 | Direct arithmetic |\n",
    "| Medium | 3-4 | Multi-step word problem |\n",
    "| Complex | 5+ | Chain reasoning required |\n",
    "\n",
    "## Conditions\n",
    "- DIRECT: No trace\n",
    "- USE: Contaminated trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP & DIRECTORIES\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_ID = 'A1_E2'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/exp_{EXPERIMENT_ID}_complexity_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/traces', exist_ok=True)\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "!pip install datasets openai anthropic pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: IMPORTS & CONFIGURATION\n",
    "# ============================================================\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "GLOBAL_SEED = 20260120\n",
    "N_PROBLEMS_PER_LEVEL = 50  # Per complexity level\n",
    "COMPLEXITY_LEVELS = ['simple', 'medium', 'complex']\n",
    "\n",
    "# Conditions\n",
    "CONDITIONS = ['DIRECT', 'USE']\n",
    "\n",
    "# Models\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT A1-E2: TASK COMPLEXITY GRADIENT')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'Complexity levels: {COMPLEXITY_LEVELS}')\n",
    "print(f'Problems per level: {N_PROBLEMS_PER_LEVEL}')\n",
    "print(f'Total problems: {N_PROBLEMS_PER_LEVEL * len(COMPLEXITY_LEVELS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: UTILITY FUNCTIONS\n",
    "# ============================================================\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    converted_data = convert_to_native(data)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "print('Utility functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: API SETUP\n",
    "# ============================================================\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 512) -> str:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: LOAD GSM8K AND CLASSIFY BY COMPLEXITY\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading GSM8K...')\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "print(f'✓ GSM8K loaded: {len(gsm8k_dataset)} problems')\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def count_reasoning_steps(answer_text: str) -> int:\n",
    "    \"\"\"Count the number of reasoning steps in GSM8K solution.\"\"\"\n",
    "    lines = answer_text.split('\\n')\n",
    "    step_count = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('####'):\n",
    "            continue\n",
    "        if re.search(r'\\d+', line) and len(line) > 10:\n",
    "            step_count += 1\n",
    "    return max(1, step_count)\n",
    "\n",
    "def classify_complexity(steps: int) -> str:\n",
    "    \"\"\"Classify problem complexity based on step count.\"\"\"\n",
    "    if steps <= 2:\n",
    "        return 'simple'\n",
    "    elif steps <= 4:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'complex'\n",
    "\n",
    "# Classify all problems\n",
    "print('\\nClassifying problems by complexity...')\n",
    "classified_problems = {'simple': [], 'medium': [], 'complex': []}\n",
    "\n",
    "for idx in range(len(gsm8k_dataset)):\n",
    "    problem = gsm8k_dataset[idx]\n",
    "    answer = extract_gsm8k_answer(problem['answer'])\n",
    "    if not answer:\n",
    "        continue\n",
    "    \n",
    "    steps = count_reasoning_steps(problem['answer'])\n",
    "    complexity = classify_complexity(steps)\n",
    "    \n",
    "    classified_problems[complexity].append({\n",
    "        'idx': idx,\n",
    "        'question': problem['question'],\n",
    "        'answer_text': problem['answer'],\n",
    "        'final_answer': answer,\n",
    "        'steps': steps\n",
    "    })\n",
    "\n",
    "print('\\nDistribution:')\n",
    "for level, problems in classified_problems.items():\n",
    "    avg_steps = np.mean([p['steps'] for p in problems]) if problems else 0\n",
    "    print(f'  {level}: {len(problems)} problems (avg {avg_steps:.1f} steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: SAMPLE PROBLEMS FOR EACH COMPLEXITY LEVEL\n",
    "# ============================================================\n",
    "\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "\n",
    "sampled_problems = {}\n",
    "for level in COMPLEXITY_LEVELS:\n",
    "    available = classified_problems[level]\n",
    "    if len(available) < N_PROBLEMS_PER_LEVEL:\n",
    "        print(f'Warning: Only {len(available)} {level} problems available')\n",
    "        sampled_problems[level] = available\n",
    "    else:\n",
    "        sampled_problems[level] = rng.sample(available, N_PROBLEMS_PER_LEVEL)\n",
    "\n",
    "print('Sampled problems:')\n",
    "for level, problems in sampled_problems.items():\n",
    "    avg_steps = np.mean([p['steps'] for p in problems]) if problems else 0\n",
    "    print(f'  {level}: {len(problems)} problems (avg {avg_steps:.1f} steps)')\n",
    "\n",
    "# Show examples\n",
    "print('\\n' + '='*50)\n",
    "print('EXAMPLES BY COMPLEXITY:')\n",
    "print('='*50)\n",
    "for level in COMPLEXITY_LEVELS:\n",
    "    if sampled_problems[level]:\n",
    "        ex = sampled_problems[level][0]\n",
    "        print(f'\\n[{level.upper()}] ({ex[\"steps\"]} steps)')\n",
    "        print(f'Q: {ex[\"question\"][:100]}...')\n",
    "        print(f'A: {ex[\"final_answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: GENERATE CONTAMINATED TRACES\n",
    "# ============================================================\n",
    "\n",
    "def generate_wrong_trace(question: str, correct_answer: str, complexity: str) -> Dict:\n",
    "    \"\"\"Generate contaminated trace with complexity-appropriate error.\"\"\"\n",
    "    \n",
    "    if complexity == 'simple':\n",
    "        error_type = \"a simple arithmetic mistake\"\n",
    "    elif complexity == 'medium':\n",
    "        error_type = \"a logical error in one of the middle steps\"\n",
    "    else:\n",
    "        error_type = \"a subtle error that compounds through multiple steps\"\n",
    "    \n",
    "    prompt = f\"\"\"Solve this math problem step by step, but make {error_type} that leads to a wrong final answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Requirements:\n",
    "1. Show clear step-by-step reasoning\n",
    "2. The error should seem plausible\n",
    "3. End with \"Therefore, the answer is [NUMBER].\"\n",
    "4. Do NOT arrive at {correct_answer} - that's the correct answer\n",
    "\n",
    "Show your (intentionally flawed) solution:\"\"\"\n",
    "\n",
    "    trace = call_api(prompt, MODELS['Claude 4 Sonnet'], max_tokens=1500)\n",
    "    \n",
    "    match = re.search(r'answer is\\s*[\\$]?([\\d,]+)', trace, re.IGNORECASE)\n",
    "    wrong_answer = match.group(1).replace(',', '') if match else \"\"\n",
    "    \n",
    "    if wrong_answer == correct_answer or not wrong_answer:\n",
    "        try:\n",
    "            wrong_num = int(correct_answer) + random.choice([10, -10, 5, -5, 20])\n",
    "            if wrong_num < 0:\n",
    "                wrong_num = int(correct_answer) + 15\n",
    "            wrong_answer = str(wrong_num)\n",
    "            trace = re.sub(r'answer is\\s*[\\$]?[\\d,]+',\n",
    "                          f'answer is {wrong_answer}',\n",
    "                          trace, flags=re.IGNORECASE)\n",
    "        except:\n",
    "            wrong_answer = str(int(correct_answer) + 10) if correct_answer.isdigit() else \"999\"\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'wrong_answer': wrong_answer,\n",
    "        'correct_answer': correct_answer\n",
    "    }\n",
    "\n",
    "# Load or initialize traces\n",
    "trace_file = f'{SAVE_DIR_EXP}/traces/all_traces.json'\n",
    "all_traces = load_json(trace_file)\n",
    "\n",
    "# 重要: 確実に全レベルを初期化\n",
    "if all_traces is None:\n",
    "    all_traces = {}\n",
    "for level in COMPLEXITY_LEVELS:\n",
    "    if level not in all_traces:\n",
    "        all_traces[level] = {}\n",
    "\n",
    "print('Generating contaminated traces...')\n",
    "total_to_generate = sum(len(sampled_problems[l]) for l in COMPLEXITY_LEVELS)\n",
    "existing_count = sum(len(all_traces.get(l, {})) for l in COMPLEXITY_LEVELS)\n",
    "print(f'Total problems: {total_to_generate}')\n",
    "print(f'Already generated: {existing_count}')\n",
    "\n",
    "generated_count = 0\n",
    "\n",
    "for level in COMPLEXITY_LEVELS:\n",
    "    for problem in tqdm(sampled_problems[level], desc=f'{level}'):\n",
    "        idx_str = str(problem['idx'])\n",
    "        if idx_str in all_traces[level]:\n",
    "            continue\n",
    "        \n",
    "        trace_data = generate_wrong_trace(\n",
    "            problem['question'],\n",
    "            problem['final_answer'],\n",
    "            level\n",
    "        )\n",
    "        all_traces[level][idx_str] = trace_data\n",
    "        generated_count += 1\n",
    "        \n",
    "        # Save periodically\n",
    "        if generated_count % 30 == 0:\n",
    "            save_json(all_traces, trace_file)\n",
    "\n",
    "save_json(all_traces, trace_file)\n",
    "print(f'\\n✓ Traces complete (generated {generated_count} new traces)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: PROMPT TEMPLATES\n",
    "# ============================================================\n",
    "\n",
    "PROMPTS = {\n",
    "    'DIRECT': \"\"\"Solve this math problem step by step.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Show your work and end with \"The answer is [NUMBER].\"\"\",\n",
    "\n",
    "    'USE': \"\"\"Solve this math problem step by step.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "An expert provided this solution:\n",
    "---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Use the expert's approach to solve the problem.\n",
    "Show your work and end with \"The answer is [NUMBER].\"\"\"\n",
    "}\n",
    "\n",
    "def extract_numerical_answer(response: str) -> str:\n",
    "    \"\"\"Extract numerical answer from response.\"\"\"\n",
    "    patterns = [\n",
    "        r'answer is\\s*[\\$]?([\\d,]+)',\n",
    "        r'Answer:\\s*[\\$]?([\\d,]+)',\n",
    "        r'=\\s*[\\$]?([\\d,]+)\\s*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Last number as fallback\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return \"\"\n",
    "\n",
    "print('Prompt templates defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: RUN EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "def run_complexity_experiment(model_name: str, model_config: dict) -> Dict:\n",
    "    \"\"\"Run experiment for a single model.\"\"\"\n",
    "    \n",
    "    short_name = model_config['short']\n",
    "    checkpoint_file = f'{SAVE_DIR_EXP}/checkpoints/results_{short_name}.json'\n",
    "    \n",
    "    results = load_json(checkpoint_file)\n",
    "    if results:\n",
    "        print(f'✓ Loaded checkpoint')\n",
    "    else:\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'problems': {level: [] for level in COMPLEXITY_LEVELS}\n",
    "        }\n",
    "    \n",
    "    # Ensure all levels exist in results\n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        if level not in results['problems']:\n",
    "            results['problems'][level] = []\n",
    "    \n",
    "    total_processed = 0\n",
    "    \n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        completed_indices = {p['idx'] for p in results['problems'][level]}\n",
    "        \n",
    "        for problem in tqdm(sampled_problems[level], desc=f'{short_name} {level}'):\n",
    "            if problem['idx'] in completed_indices:\n",
    "                continue\n",
    "            \n",
    "            idx_str = str(problem['idx'])\n",
    "            if idx_str not in all_traces[level]:\n",
    "                print(f'Warning: No trace for problem {idx_str}')\n",
    "                continue\n",
    "            \n",
    "            trace_data = all_traces[level][idx_str]\n",
    "            \n",
    "            problem_result = {\n",
    "                'idx': problem['idx'],\n",
    "                'steps': problem['steps'],\n",
    "                'correct_answer': problem['final_answer'],\n",
    "                'wrong_answer': trace_data['wrong_answer'],\n",
    "                'responses': {}\n",
    "            }\n",
    "            \n",
    "            for condition in CONDITIONS:\n",
    "                prompt = PROMPTS[condition].format(\n",
    "                    question=problem['question'],\n",
    "                    trace=trace_data['trace']\n",
    "                )\n",
    "                \n",
    "                response = call_api(prompt, model_config, max_tokens=1000)\n",
    "                extracted = extract_numerical_answer(response)\n",
    "                \n",
    "                problem_result['responses'][condition] = {\n",
    "                    'raw': response[:500],\n",
    "                    'extracted': extracted,\n",
    "                    'correct': extracted == problem['final_answer'],\n",
    "                    'followed_wrong': extracted == trace_data['wrong_answer']\n",
    "                }\n",
    "            \n",
    "            results['problems'][level].append(problem_result)\n",
    "            total_processed += 1\n",
    "            \n",
    "            # Save periodically\n",
    "            if total_processed % 20 == 0:\n",
    "                save_json(results, checkpoint_file)\n",
    "    \n",
    "    save_json(results, checkpoint_file)\n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "print('\\n' + '='*60)\n",
    "print('RUNNING COMPLEXITY EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "all_results = {}\n",
    "for model_name, model_config in MODELS.items():\n",
    "    print(f'\\n--- {model_name} ---')\n",
    "    all_results[model_config['short']] = run_complexity_experiment(model_name, model_config)\n",
    "\n",
    "print('\\n✓ Experiment complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: ANALYZE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_by_complexity(results: Dict) -> Dict:\n",
    "    \"\"\"Analyze results by complexity level.\"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        problems = results['problems'].get(level, [])\n",
    "        n = len(problems)\n",
    "        \n",
    "        if n == 0:\n",
    "            analysis[level] = {'n': 0, 'error': 'No data'}\n",
    "            continue\n",
    "        \n",
    "        level_analysis = {\n",
    "            'n': n,\n",
    "            'avg_steps': np.mean([p['steps'] for p in problems]),\n",
    "            'accuracy': {},\n",
    "            'cif_rate': 0,\n",
    "            'followed_wrong_in_cif': 0\n",
    "        }\n",
    "        \n",
    "        for cond in CONDITIONS:\n",
    "            correct = sum(1 for p in problems if p['responses'][cond]['correct'])\n",
    "            level_analysis['accuracy'][cond] = correct / n\n",
    "        \n",
    "        # CIF: Direct correct → USE wrong\n",
    "        direct_correct = [p for p in problems if p['responses']['DIRECT']['correct']]\n",
    "        cif_cases = [p for p in direct_correct if not p['responses']['USE']['correct']]\n",
    "        \n",
    "        level_analysis['cif_rate'] = len(cif_cases) / len(direct_correct) if direct_correct else 0\n",
    "        level_analysis['n_direct_correct'] = len(direct_correct)\n",
    "        level_analysis['n_cif'] = len(cif_cases)\n",
    "        \n",
    "        followed = sum(1 for p in cif_cases if p['responses']['USE']['followed_wrong'])\n",
    "        level_analysis['followed_wrong_in_cif'] = followed / len(cif_cases) if cif_cases else 0\n",
    "        \n",
    "        analysis[level] = level_analysis\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze\n",
    "print('\\n' + '='*60)\n",
    "print('RESULTS BY COMPLEXITY')\n",
    "print('='*60)\n",
    "\n",
    "all_analyses = {}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_results:\n",
    "        continue\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{model_name}')\n",
    "    print('-'*50)\n",
    "    \n",
    "    analysis = analyze_by_complexity(all_results[model_key])\n",
    "    all_analyses[model_key] = analysis\n",
    "    \n",
    "    print(f'{\"Level\":<10} {\"Steps\":<8} {\"DIRECT\":<10} {\"USE\":<10} {\"CIF\":<10} {\"Follow%\":<10}')\n",
    "    print('-'*58)\n",
    "    \n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        a = analysis.get(level, {})\n",
    "        if 'error' in a or a.get('n', 0) == 0:\n",
    "            print(f'{level:<10} No data')\n",
    "            continue\n",
    "        print(f'{level:<10} {a[\"avg_steps\"]:>5.1f}   {a[\"accuracy\"][\"DIRECT\"]:>7.1%}   '\n",
    "              f'{a[\"accuracy\"][\"USE\"]:>7.1%}   {a[\"cif_rate\"]:>7.1%}   '\n",
    "              f'{a[\"followed_wrong_in_cif\"]:>7.1%}')\n",
    "\n",
    "save_json(all_analyses, f'{SAVE_DIR_EXP}/results/analysis_by_complexity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: STATISTICAL ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('STATISTICAL ANALYSIS: COMPLEXITY → CIF')\n",
    "print('='*60)\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_results:\n",
    "        continue\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{model_name}')\n",
    "    print('-'*50)\n",
    "    \n",
    "    # Collect step counts and CIF outcomes per problem\n",
    "    steps_list = []\n",
    "    cif_list = []\n",
    "    \n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        for p in all_results[model_key]['problems'].get(level, []):\n",
    "            if p['responses']['DIRECT']['correct']:\n",
    "                steps_list.append(p['steps'])\n",
    "                cif_occurred = 1 if not p['responses']['USE']['correct'] else 0\n",
    "                cif_list.append(cif_occurred)\n",
    "    \n",
    "    if len(steps_list) > 10 and len(set(cif_list)) > 1:\n",
    "        r, p_value = stats.pointbiserialr(cif_list, steps_list)\n",
    "        \n",
    "        print(f'  N (direct correct): {len(steps_list)}')\n",
    "        print(f'  Correlation (steps vs CIF): r = {r:.3f}')\n",
    "        print(f'  p-value: {p_value:.4f}')\n",
    "        print(f'  Significant: {\"Yes\" if p_value < 0.05 else \"No\"}')\n",
    "        \n",
    "        correlation_results[model_key] = {\n",
    "            'n': len(steps_list),\n",
    "            'correlation': r,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    else:\n",
    "        print('  Insufficient data for correlation analysis')\n",
    "        correlation_results[model_key] = {'error': 'Insufficient data'}\n",
    "\n",
    "# Trend analysis\n",
    "print('\\n' + '='*60)\n",
    "print('CIF RATE TREND BY COMPLEXITY')\n",
    "print('='*60)\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    analysis = all_analyses[model_key]\n",
    "    cif_rates = []\n",
    "    for level in COMPLEXITY_LEVELS:\n",
    "        a = analysis.get(level, {})\n",
    "        if 'cif_rate' in a:\n",
    "            cif_rates.append(a['cif_rate'])\n",
    "    \n",
    "    if len(cif_rates) == 3:\n",
    "        trend = 'increasing' if cif_rates[2] > cif_rates[0] else 'decreasing'\n",
    "        delta = cif_rates[2] - cif_rates[0]\n",
    "        print(f'{model_key}: {cif_rates[0]:.1%} → {cif_rates[1]:.1%} → {cif_rates[2]:.1%} ({trend}, Δ={delta:+.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {'sonnet4': '#5B8FF9', 'gpt4o': '#5AD8A6'}\n",
    "model_labels = {'sonnet4': 'Claude 4 Sonnet', 'gpt4o': 'GPT-4o'}\n",
    "\n",
    "# Plot 1: CIF Rate by Complexity\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(COMPLEXITY_LEVELS))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    cif_rates = [all_analyses[model_key].get(level, {}).get('cif_rate', 0) \n",
    "                 for level in COMPLEXITY_LEVELS]\n",
    "    ax1.bar(x + i*width, cif_rates, width, \n",
    "            label=model_labels[model_key], color=colors[model_key])\n",
    "\n",
    "ax1.set_ylabel('CIF Rate', fontsize=12)\n",
    "ax1.set_title('CIF Rate by Task Complexity', fontsize=14)\n",
    "ax1.set_xticks(x + width/2)\n",
    "ax1.set_xticklabels(['Simple\\n(1-2 steps)', 'Medium\\n(3-4 steps)', 'Complex\\n(5+ steps)'])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "ax2 = axes[1]\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    direct_acc = [all_analyses[model_key].get(level, {}).get('accuracy', {}).get('DIRECT', 0) \n",
    "                  for level in COMPLEXITY_LEVELS]\n",
    "    use_acc = [all_analyses[model_key].get(level, {}).get('accuracy', {}).get('USE', 0) \n",
    "               for level in COMPLEXITY_LEVELS]\n",
    "    \n",
    "    offset = (i - 0.5) * width\n",
    "    ax2.bar(x + offset - width/4, direct_acc, width/2, \n",
    "            label=f'{model_labels[model_key]} DIRECT', \n",
    "            color=colors[model_key], alpha=0.5)\n",
    "    ax2.bar(x + offset + width/4, use_acc, width/2,\n",
    "            label=f'{model_labels[model_key]} USE', \n",
    "            color=colors[model_key], alpha=1.0)\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy by Complexity & Condition', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Simple', 'Medium', 'Complex'])\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3: Follow-wrong rate in CIF cases\n",
    "ax3 = axes[2]\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    follow_rates = [all_analyses[model_key].get(level, {}).get('followed_wrong_in_cif', 0) \n",
    "                    for level in COMPLEXITY_LEVELS]\n",
    "    ax3.bar(x + i*width, follow_rates, width,\n",
    "            label=model_labels[model_key], color=colors[model_key])\n",
    "\n",
    "ax3.set_ylabel('Follow-Wrong Rate (in CIF)', fontsize=12)\n",
    "ax3.set_title('How Often CIF Follows Trace Answer', fontsize=14)\n",
    "ax3.set_xticks(x + width/2)\n",
    "ax3.set_xticklabels(['Simple', 'Medium', 'Complex'])\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_EXP}/exp_A1_E2_complexity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n✓ Figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "summary = {\n",
    "    'experiment_id': 'A1_E2',\n",
    "    'experiment_name': 'Task Complexity Gradient',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'hypothesis': 'Complex tasks (more steps) show higher CIF than simple tasks',\n",
    "    'design': {\n",
    "        'simple': '1-2 reasoning steps',\n",
    "        'medium': '3-4 reasoning steps',\n",
    "        'complex': '5+ reasoning steps'\n",
    "    },\n",
    "    'n_problems_per_level': N_PROBLEMS_PER_LEVEL,\n",
    "    'models': list(MODELS.keys()),\n",
    "    'results': all_analyses,\n",
    "    'correlation_analysis': correlation_results,\n",
    "    'key_findings': []\n",
    "}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    analysis = all_analyses[model_key]\n",
    "    cif_simple = analysis.get('simple', {}).get('cif_rate', 0)\n",
    "    cif_complex = analysis.get('complex', {}).get('cif_rate', 0)\n",
    "    \n",
    "    summary['key_findings'].append({\n",
    "        'model': model_key,\n",
    "        'cif_simple': cif_simple,\n",
    "        'cif_complex': cif_complex,\n",
    "        'cif_increase': cif_complex - cif_simple,\n",
    "        'supports_hypothesis': cif_complex > cif_simple + 0.05,\n",
    "        'correlation': correlation_results.get(model_key, {})\n",
    "    })\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_EXP}/results/exp_A1_E2_summary.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('EXPERIMENT A1-E2 COMPLETE')\n",
    "print('='*60)\n",
    "print(f'\\nResults saved to: {SAVE_DIR_EXP}')\n",
    "print('\\n' + '='*60)\n",
    "print('KEY FINDINGS')\n",
    "print('='*60)\n",
    "\n",
    "for finding in summary['key_findings']:\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == finding['model']][0]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Simple CIF:  {finding['cif_simple']:.1%}\")\n",
    "    print(f\"  Complex CIF: {finding['cif_complex']:.1%}\")\n",
    "    print(f\"  Δ CIF: {finding['cif_increase']:+.1%}\")\n",
    "    print(f\"  Supports hypothesis: {'✓ YES' if finding['supports_hypothesis'] else '? No'}\")\n",
    "    corr = finding.get('correlation', {})\n",
    "    if 'correlation' in corr:\n",
    "        print(f\"  Correlation: r={corr['correlation']:.3f}, p={corr['p_value']:.4f}\")\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
