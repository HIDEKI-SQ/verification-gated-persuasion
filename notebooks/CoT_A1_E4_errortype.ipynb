{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT A1-E4: Error Type Taxonomy\n",
    "\n",
    "## Purpose\n",
    "Test whether the **type of error** in contaminated traces affects CIF vulnerability.\n",
    "\n",
    "## Hypothesis\n",
    "- Easy-to-detect errors (arithmetic): Low CIF - model catches the mistake\n",
    "- Hard-to-detect errors (conceptual/setup): High CIF - model follows flawed logic\n",
    "\n",
    "## Design\n",
    "| Error Type | Description | Detectability |\n",
    "|------------|-------------|---------------|\n",
    "| Arithmetic | 5+3=9 type mistakes | Easy |\n",
    "| Computational | Order of operations errors | Medium |\n",
    "| Setup | Wrong equation/relationship | Hard |\n",
    "| Conceptual | Misunderstanding the problem | Hard |\n",
    "\n",
    "## Key Question\n",
    "Which types of reasoning errors most effectively \"fool\" language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP & DIRECTORIES\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_ID = 'A1_E4'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/exp_{EXPERIMENT_ID}_errortype_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/traces', exist_ok=True)\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "!pip install datasets openai anthropic pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: IMPORTS & CONFIGURATION\n",
    "# ============================================================\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "GLOBAL_SEED = 20260120\n",
    "N_PROBLEMS_PER_TYPE = 40  # Per error type\n",
    "\n",
    "# Error types with descriptions and prompts\n",
    "ERROR_TYPES = {\n",
    "    'arithmetic': {\n",
    "        'description': 'Simple calculation mistake (e.g., 5+3=9)',\n",
    "        'detectability': 'easy',\n",
    "        'prompt_instruction': 'make a simple arithmetic calculation error (like adding wrong, e.g., saying 5+3=9)'\n",
    "    },\n",
    "    'computational': {\n",
    "        'description': 'Order of operations or multi-step calculation error',\n",
    "        'detectability': 'medium',\n",
    "        'prompt_instruction': 'make an error in the order of operations or in combining multiple calculation steps'\n",
    "    },\n",
    "    'setup': {\n",
    "        'description': 'Wrong equation or relationship setup',\n",
    "        'detectability': 'hard',\n",
    "        'prompt_instruction': 'set up the wrong equation or mathematical relationship for the problem (the math itself should be correct, but the setup is wrong)'\n",
    "    },\n",
    "    'conceptual': {\n",
    "        'description': 'Misunderstanding what the problem asks',\n",
    "        'detectability': 'hard',\n",
    "        'prompt_instruction': 'misunderstand what the problem is asking (e.g., calculate the wrong quantity or misinterpret a condition)'\n",
    "    }\n",
    "}\n",
    "\n",
    "ERROR_TYPE_NAMES = list(ERROR_TYPES.keys())\n",
    "\n",
    "# Conditions\n",
    "CONDITIONS = ['DIRECT', 'USE']\n",
    "\n",
    "# Models\n",
    "MODELS = {\n",
    "    'Claude Sonnet 4': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT A1-E4: ERROR TYPE TAXONOMY')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'Error types: {ERROR_TYPE_NAMES}')\n",
    "print(f'Problems per type: {N_PROBLEMS_PER_TYPE}')\n",
    "print(f'Total problems: {N_PROBLEMS_PER_TYPE * len(ERROR_TYPE_NAMES)}')\n",
    "print(f'\\nError type details:')\n",
    "for etype, info in ERROR_TYPES.items():\n",
    "    print(f'  {etype}: {info[\"description\"]} (detectability: {info[\"detectability\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: UTILITY FUNCTIONS\n",
    "# ============================================================\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy/pandas types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    \"\"\"Save data to JSON file with type conversion.\"\"\"\n",
    "    converted_data = convert_to_native(data)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"Load JSON file if it exists.\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "print('Utility functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: API SETUP\n",
    "# ============================================================\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Call API with retry logic.\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: LOAD DATASET\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading GSM8K...')\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "print(f'✓ GSM8K loaded: {len(gsm8k_dataset)} problems')\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract final numerical answer from GSM8K format.\"\"\"\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "# Sample problems - need enough for all error types\n",
    "total_needed = N_PROBLEMS_PER_TYPE * len(ERROR_TYPE_NAMES)\n",
    "\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "indices = list(range(len(gsm8k_dataset)))\n",
    "rng.shuffle(indices)\n",
    "selected_indices = indices[:total_needed + 20]  # Extra buffer\n",
    "\n",
    "all_problems = []\n",
    "for idx in selected_indices:\n",
    "    item = gsm8k_dataset[idx]\n",
    "    answer = extract_gsm8k_answer(item['answer'])\n",
    "    if answer:\n",
    "        all_problems.append({\n",
    "            'idx': idx,\n",
    "            'question': item['question'],\n",
    "            'answer_text': item['answer'],\n",
    "            'final_answer': answer\n",
    "        })\n",
    "    if len(all_problems) >= total_needed:\n",
    "        break\n",
    "\n",
    "print(f'\\n✓ Selected {len(all_problems)} problems total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: ASSIGN PROBLEMS TO ERROR TYPES\n",
    "# ============================================================\n",
    "\n",
    "# Distribute problems across error types\n",
    "problems_by_type = {etype: [] for etype in ERROR_TYPE_NAMES}\n",
    "\n",
    "for i, problem in enumerate(all_problems):\n",
    "    error_type = ERROR_TYPE_NAMES[i % len(ERROR_TYPE_NAMES)]\n",
    "    if len(problems_by_type[error_type]) < N_PROBLEMS_PER_TYPE:\n",
    "        problems_by_type[error_type].append(problem)\n",
    "\n",
    "print('Problems assigned to error types:')\n",
    "for etype, problems in problems_by_type.items():\n",
    "    print(f'  {etype}: {len(problems)} problems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: GENERATE ERROR-TYPE-SPECIFIC CONTAMINATED TRACES\n",
    "# ============================================================\n",
    "\n",
    "def generate_typed_wrong_trace(question: str, correct_answer: str, error_type: str) -> Dict:\n",
    "    \"\"\"Generate a contaminated trace with a specific type of error.\"\"\"\n",
    "    \n",
    "    error_instruction = ERROR_TYPES[error_type]['prompt_instruction']\n",
    "    \n",
    "    prompt = f\"\"\"Solve this math problem step by step, but {error_instruction}.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Requirements:\n",
    "1. Show clear step-by-step reasoning\n",
    "2. The error should be of type: {error_type}\n",
    "3. The error should seem plausible (not obviously wrong)\n",
    "4. End with \"Therefore, the answer is [NUMBER].\"\n",
    "5. Do NOT arrive at {correct_answer} - that's the correct answer\n",
    "\n",
    "Show your (intentionally flawed) solution:\"\"\"\n",
    "\n",
    "    trace = call_api(prompt, MODELS['Claude Sonnet 4'], max_tokens=1500)\n",
    "    \n",
    "    match = re.search(r'answer is\\s*[\\$]?([\\d,]+)', trace, re.IGNORECASE)\n",
    "    wrong_answer = match.group(1).replace(',', '') if match else \"\"\n",
    "    \n",
    "    # Ensure answer is actually wrong\n",
    "    if wrong_answer == correct_answer or not wrong_answer:\n",
    "        try:\n",
    "            # Different offsets for different error types to add variety\n",
    "            offset_map = {'arithmetic': 1, 'computational': 5, 'setup': 10, 'conceptual': 20}\n",
    "            offset = offset_map.get(error_type, 10) + random.randint(-3, 3)\n",
    "            wrong_num = int(correct_answer) + offset\n",
    "            if wrong_num < 0:\n",
    "                wrong_num = abs(wrong_num) + 5\n",
    "            wrong_answer = str(wrong_num)\n",
    "            trace = re.sub(r'answer is\\s*[\\$]?[\\d,]+',\n",
    "                          f'answer is {wrong_answer}',\n",
    "                          trace, flags=re.IGNORECASE)\n",
    "        except:\n",
    "            wrong_answer = str(int(correct_answer) + 10) if correct_answer.isdigit() else \"999\"\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'wrong_answer': wrong_answer,\n",
    "        'correct_answer': correct_answer,\n",
    "        'error_type': error_type\n",
    "    }\n",
    "\n",
    "# Load or initialize traces\n",
    "trace_file = f'{SAVE_DIR_EXP}/traces/typed_traces.json'\n",
    "all_traces = load_json(trace_file)\n",
    "\n",
    "# Initialize structure if needed\n",
    "if all_traces is None:\n",
    "    all_traces = {}\n",
    "\n",
    "for etype in ERROR_TYPE_NAMES:\n",
    "    if etype not in all_traces:\n",
    "        all_traces[etype] = {}\n",
    "\n",
    "print('Generating error-type-specific traces...')\n",
    "total_to_generate = sum(len(problems_by_type[t]) for t in ERROR_TYPE_NAMES)\n",
    "existing_count = sum(len(all_traces.get(t, {})) for t in ERROR_TYPE_NAMES)\n",
    "print(f'Total needed: {total_to_generate}')\n",
    "print(f'Already have: {existing_count}')\n",
    "\n",
    "generated_count = 0\n",
    "\n",
    "for error_type in ERROR_TYPE_NAMES:\n",
    "    for problem in tqdm(problems_by_type[error_type], desc=f'{error_type}'):\n",
    "        idx_str = str(problem['idx'])\n",
    "        \n",
    "        if idx_str in all_traces[error_type]:\n",
    "            continue\n",
    "        \n",
    "        trace_data = generate_typed_wrong_trace(\n",
    "            problem['question'],\n",
    "            problem['final_answer'],\n",
    "            error_type\n",
    "        )\n",
    "        all_traces[error_type][idx_str] = trace_data\n",
    "        generated_count += 1\n",
    "        \n",
    "        # Save periodically\n",
    "        if generated_count % 20 == 0:\n",
    "            save_json(all_traces, trace_file)\n",
    "\n",
    "save_json(all_traces, trace_file)\n",
    "print(f'\\n✓ Traces complete (generated {generated_count} new traces)')\n",
    "\n",
    "# Show examples\n",
    "print('\\n' + '='*50)\n",
    "print('EXAMPLE TRACES BY ERROR TYPE')\n",
    "print('='*50)\n",
    "for etype in ERROR_TYPE_NAMES:\n",
    "    if all_traces[etype]:\n",
    "        ex_idx = list(all_traces[etype].keys())[0]\n",
    "        ex = all_traces[etype][ex_idx]\n",
    "        print(f'\\n[{etype.upper()}]')\n",
    "        print(f'  Correct: {ex[\"correct_answer\"]}')\n",
    "        print(f'  Wrong: {ex[\"wrong_answer\"]}')\n",
    "        print(f'  Trace snippet: {ex[\"trace\"][:150]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: PROMPT TEMPLATES\n",
    "# ============================================================\n",
    "\n",
    "PROMPTS = {\n",
    "    'DIRECT': \"\"\"Solve this math problem step by step.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Show your work and end with \"The answer is [NUMBER].\"\"\",\n",
    "\n",
    "    'USE': \"\"\"Solve this math problem step by step.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "An expert provided this solution:\n",
    "---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Use the expert's approach to solve the problem.\n",
    "Show your work and end with \"The answer is [NUMBER].\"\"\"\n",
    "}\n",
    "\n",
    "def extract_numerical_answer(response: str) -> str:\n",
    "    \"\"\"Extract numerical answer from response.\"\"\"\n",
    "    patterns = [\n",
    "        r'answer is\\s*[\\$]?([\\d,]+)',\n",
    "        r'Answer:\\s*[\\$]?([\\d,]+)',\n",
    "        r'=\\s*[\\$]?([\\d,]+)\\s*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return \"\"\n",
    "\n",
    "print('Prompt templates defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: RUN EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "def run_errortype_experiment(model_name: str, model_config: dict) -> Dict:\n",
    "    \"\"\"Run experiment for a single model across all error types.\"\"\"\n",
    "    \n",
    "    short_name = model_config['short']\n",
    "    checkpoint_file = f'{SAVE_DIR_EXP}/checkpoints/results_{short_name}.json'\n",
    "    \n",
    "    results = load_json(checkpoint_file)\n",
    "    if results:\n",
    "        print(f'  ✓ Loaded checkpoint')\n",
    "    else:\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'problems': {etype: [] for etype in ERROR_TYPE_NAMES}\n",
    "        }\n",
    "    \n",
    "    # Ensure all error types exist\n",
    "    for etype in ERROR_TYPE_NAMES:\n",
    "        if etype not in results['problems']:\n",
    "            results['problems'][etype] = []\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    for error_type in ERROR_TYPE_NAMES:\n",
    "        completed_indices = {p['idx'] for p in results['problems'][error_type]}\n",
    "        \n",
    "        for problem in tqdm(problems_by_type[error_type], \n",
    "                           desc=f'{short_name} {error_type}', leave=False):\n",
    "            if problem['idx'] in completed_indices:\n",
    "                continue\n",
    "            \n",
    "            idx_str = str(problem['idx'])\n",
    "            if idx_str not in all_traces[error_type]:\n",
    "                print(f'Warning: No trace for problem {idx_str}')\n",
    "                continue\n",
    "            \n",
    "            trace_data = all_traces[error_type][idx_str]\n",
    "            \n",
    "            problem_result = {\n",
    "                'idx': problem['idx'],\n",
    "                'error_type': error_type,\n",
    "                'correct_answer': problem['final_answer'],\n",
    "                'wrong_answer': trace_data['wrong_answer'],\n",
    "                'responses': {}\n",
    "            }\n",
    "            \n",
    "            for condition in CONDITIONS:\n",
    "                prompt = PROMPTS[condition].format(\n",
    "                    question=problem['question'],\n",
    "                    trace=trace_data['trace']\n",
    "                )\n",
    "                \n",
    "                response = call_api(prompt, model_config, max_tokens=1000)\n",
    "                extracted = extract_numerical_answer(response)\n",
    "                \n",
    "                problem_result['responses'][condition] = {\n",
    "                    'raw': response[:500],\n",
    "                    'extracted': extracted,\n",
    "                    'correct': extracted == problem['final_answer'],\n",
    "                    'followed_wrong': extracted == trace_data['wrong_answer']\n",
    "                }\n",
    "            \n",
    "            results['problems'][error_type].append(problem_result)\n",
    "            processed_count += 1\n",
    "            \n",
    "            if processed_count % 20 == 0:\n",
    "                save_json(results, checkpoint_file)\n",
    "    \n",
    "    save_json(results, checkpoint_file)\n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "print('\\n' + '='*60)\n",
    "print('RUNNING ERROR TYPE EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "all_results = {}\n",
    "for model_name, model_config in MODELS.items():\n",
    "    print(f'\\n--- {model_name} ---')\n",
    "    all_results[model_config['short']] = run_errortype_experiment(model_name, model_config)\n",
    "\n",
    "print('\\n✓ Experiment complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: ANALYZE RESULTS BY ERROR TYPE\n",
    "# ============================================================\n",
    "\n",
    "def analyze_by_error_type(results: Dict) -> Dict:\n",
    "    \"\"\"Analyze results for each error type.\"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    for error_type in ERROR_TYPE_NAMES:\n",
    "        problems = results['problems'].get(error_type, [])\n",
    "        n = len(problems)\n",
    "        \n",
    "        if n == 0:\n",
    "            analysis[error_type] = {'n': 0, 'error': 'No data'}\n",
    "            continue\n",
    "        \n",
    "        type_analysis = {\n",
    "            'n': n,\n",
    "            'detectability': ERROR_TYPES[error_type]['detectability'],\n",
    "            'accuracy': {},\n",
    "            'cif_rate': 0,\n",
    "            'followed_wrong_in_cif': 0\n",
    "        }\n",
    "        \n",
    "        # Accuracy per condition\n",
    "        for cond in CONDITIONS:\n",
    "            correct = sum(1 for p in problems if p['responses'][cond]['correct'])\n",
    "            type_analysis['accuracy'][cond] = correct / n\n",
    "        \n",
    "        # CIF analysis\n",
    "        direct_correct = [p for p in problems if p['responses']['DIRECT']['correct']]\n",
    "        cif_cases = [p for p in direct_correct if not p['responses']['USE']['correct']]\n",
    "        \n",
    "        type_analysis['n_direct_correct'] = len(direct_correct)\n",
    "        type_analysis['n_cif'] = len(cif_cases)\n",
    "        type_analysis['cif_rate'] = len(cif_cases) / len(direct_correct) if direct_correct else 0\n",
    "        \n",
    "        followed = sum(1 for p in cif_cases if p['responses']['USE']['followed_wrong'])\n",
    "        type_analysis['followed_wrong_in_cif'] = followed / len(cif_cases) if cif_cases else 0\n",
    "        \n",
    "        analysis[error_type] = type_analysis\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze\n",
    "print('\\n' + '='*60)\n",
    "print('RESULTS BY ERROR TYPE')\n",
    "print('='*60)\n",
    "\n",
    "all_analyses = {}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_results:\n",
    "        continue\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{model_name}')\n",
    "    print('-'*60)\n",
    "    \n",
    "    analysis = analyze_by_error_type(all_results[model_key])\n",
    "    all_analyses[model_key] = analysis\n",
    "    \n",
    "    print(f'{\"Error Type\":<15} {\"Detect\":<8} {\"DIRECT\":<10} {\"USE\":<10} {\"CIF\":<10} {\"Follow%\":<10}')\n",
    "    print('-'*63)\n",
    "    \n",
    "    for etype in ERROR_TYPE_NAMES:\n",
    "        a = analysis.get(etype, {})\n",
    "        if 'error' in a or a.get('n', 0) == 0:\n",
    "            print(f'{etype:<15} No data')\n",
    "            continue\n",
    "        print(f'{etype:<15} {a[\"detectability\"]:<8} '\n",
    "              f'{a[\"accuracy\"][\"DIRECT\"]:>7.1%}   '\n",
    "              f'{a[\"accuracy\"][\"USE\"]:>7.1%}   '\n",
    "              f'{a[\"cif_rate\"]:>7.1%}   '\n",
    "              f'{a[\"followed_wrong_in_cif\"]:>7.1%}')\n",
    "\n",
    "save_json(all_analyses, f'{SAVE_DIR_EXP}/results/analysis_by_errortype.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: STATISTICAL ANALYSIS - DETECTABILITY VS CIF\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('STATISTICAL ANALYSIS: DETECTABILITY → CIF')\n",
    "print('='*60)\n",
    "\n",
    "# Map detectability to numeric values\n",
    "detectability_map = {'easy': 1, 'medium': 2, 'hard': 3}\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{model_name}')\n",
    "    print('-'*50)\n",
    "    \n",
    "    # Collect detectability and CIF rates\n",
    "    detectabilities = []\n",
    "    cif_rates = []\n",
    "    \n",
    "    for etype in ERROR_TYPE_NAMES:\n",
    "        a = all_analyses[model_key].get(etype, {})\n",
    "        if 'cif_rate' in a:\n",
    "            detectabilities.append(detectability_map[ERROR_TYPES[etype]['detectability']])\n",
    "            cif_rates.append(a['cif_rate'])\n",
    "    \n",
    "    if len(detectabilities) >= 3:\n",
    "        # Spearman correlation (ordinal data)\n",
    "        r, p_value = stats.spearmanr(detectabilities, cif_rates)\n",
    "        \n",
    "        print(f'  Detectability levels: {detectabilities}')\n",
    "        print(f'  CIF rates: {[f\"{c:.1%}\" for c in cif_rates]}')\n",
    "        print(f'  Spearman correlation: r = {r:.3f}')\n",
    "        print(f'  p-value: {p_value:.4f}')\n",
    "        print(f'  Direction: {\"Higher detectability difficulty → Higher CIF\" if r > 0 else \"Lower CIF\"}')\n",
    "        \n",
    "        correlation_results[model_key] = {\n",
    "            'correlation': r,\n",
    "            'p_value': p_value,\n",
    "            'direction': 'positive' if r > 0 else 'negative'\n",
    "        }\n",
    "    else:\n",
    "        print('  Insufficient data for correlation')\n",
    "\n",
    "# Compare easy vs hard\n",
    "print('\\n' + '='*60)\n",
    "print('EASY vs HARD ERROR COMPARISON')\n",
    "print('='*60)\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    \n",
    "    easy_cif = all_analyses[model_key].get('arithmetic', {}).get('cif_rate', None)\n",
    "    hard_cifs = [\n",
    "        all_analyses[model_key].get(et, {}).get('cif_rate', None)\n",
    "        for et in ['setup', 'conceptual']\n",
    "    ]\n",
    "    hard_cifs = [c for c in hard_cifs if c is not None]\n",
    "    \n",
    "    if easy_cif is not None and hard_cifs:\n",
    "        hard_avg = np.mean(hard_cifs)\n",
    "        print(f'{model_key}:')\n",
    "        print(f'  Easy (arithmetic) CIF: {easy_cif:.1%}')\n",
    "        print(f'  Hard (setup/conceptual) avg CIF: {hard_avg:.1%}')\n",
    "        print(f'  Difference: {hard_avg - easy_cif:+.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {'sonnet4': '#5B8FF9', 'gpt4o': '#5AD8A6'}\n",
    "model_labels = {'sonnet4': 'Claude Sonnet 4', 'gpt4o': 'GPT-4o'}\n",
    "detectability_colors = {'easy': '#2ECC71', 'medium': '#F39C12', 'hard': '#E74C3C'}\n",
    "\n",
    "# Plot 1: CIF Rate by Error Type\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(ERROR_TYPE_NAMES))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    cif_rates = [\n",
    "        all_analyses[model_key].get(et, {}).get('cif_rate', 0)\n",
    "        for et in ERROR_TYPE_NAMES\n",
    "    ]\n",
    "    ax1.bar(x + i*width, cif_rates, width, \n",
    "            label=model_labels[model_key], color=colors[model_key])\n",
    "\n",
    "ax1.set_ylabel('CIF Rate', fontsize=12)\n",
    "ax1.set_title('CIF Rate by Error Type', fontsize=14)\n",
    "ax1.set_xticks(x + width/2)\n",
    "ax1.set_xticklabels([et.capitalize() for et in ERROR_TYPE_NAMES], fontsize=10)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add detectability indicators\n",
    "for i, et in enumerate(ERROR_TYPE_NAMES):\n",
    "    detect = ERROR_TYPES[et]['detectability']\n",
    "    ax1.annotate(f'({detect})', (i + width/2, -0.08), \n",
    "                ha='center', fontsize=8, color=detectability_colors[detect])\n",
    "\n",
    "# Plot 2: Followed-Wrong Rate by Error Type\n",
    "ax2 = axes[1]\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    follow_rates = [\n",
    "        all_analyses[model_key].get(et, {}).get('followed_wrong_in_cif', 0)\n",
    "        for et in ERROR_TYPE_NAMES\n",
    "    ]\n",
    "    ax2.bar(x + i*width, follow_rates, width,\n",
    "            label=model_labels[model_key], color=colors[model_key])\n",
    "\n",
    "ax2.set_ylabel('Followed-Wrong Rate (in CIF)', fontsize=12)\n",
    "ax2.set_title('How Often CIF Follows Trace Answer', fontsize=14)\n",
    "ax2.set_xticks(x + width/2)\n",
    "ax2.set_xticklabels([et.capitalize() for et in ERROR_TYPE_NAMES], fontsize=10)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3: CIF Rate by Detectability (grouped)\n",
    "ax3 = axes[2]\n",
    "\n",
    "detect_groups = {'easy': ['arithmetic'], 'medium': ['computational'], 'hard': ['setup', 'conceptual']}\n",
    "detect_labels = ['Easy', 'Medium', 'Hard']\n",
    "x = np.arange(len(detect_labels))\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    grouped_cif = []\n",
    "    for detect in ['easy', 'medium', 'hard']:\n",
    "        etypes = detect_groups[detect]\n",
    "        rates = [all_analyses[model_key].get(et, {}).get('cif_rate', 0) for et in etypes]\n",
    "        grouped_cif.append(np.mean(rates) if rates else 0)\n",
    "    \n",
    "    ax3.bar(x + i*width, grouped_cif, width,\n",
    "            label=model_labels[model_key], color=colors[model_key])\n",
    "\n",
    "ax3.set_ylabel('Average CIF Rate', fontsize=12)\n",
    "ax3.set_title('CIF Rate by Error Detectability', fontsize=14)\n",
    "ax3.set_xticks(x + width/2)\n",
    "ax3.set_xticklabels(detect_labels)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Color the x-labels\n",
    "for i, (label, detect) in enumerate(zip(detect_labels, ['easy', 'medium', 'hard'])):\n",
    "    ax3.get_xticklabels()[i].set_color(detectability_colors[detect])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_EXP}/exp_A1_E4_errortype.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n✓ Figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "summary = {\n",
    "    'experiment_id': 'A1_E4',\n",
    "    'experiment_name': 'Error Type Taxonomy',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'hypothesis': 'Hard-to-detect errors cause higher CIF than easy-to-detect errors',\n",
    "    'error_types': {et: ERROR_TYPES[et] for et in ERROR_TYPE_NAMES},\n",
    "    'n_problems_per_type': N_PROBLEMS_PER_TYPE,\n",
    "    'models': list(MODELS.keys()),\n",
    "    'results': all_analyses,\n",
    "    'correlation_results': correlation_results,\n",
    "    'key_findings': []\n",
    "}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    if model_key not in all_analyses:\n",
    "        continue\n",
    "    \n",
    "    analysis = all_analyses[model_key]\n",
    "    \n",
    "    # Get CIF by detectability\n",
    "    easy_cif = analysis.get('arithmetic', {}).get('cif_rate', None)\n",
    "    medium_cif = analysis.get('computational', {}).get('cif_rate', None)\n",
    "    hard_cifs = [analysis.get(et, {}).get('cif_rate', None) for et in ['setup', 'conceptual']]\n",
    "    hard_cif = np.mean([c for c in hard_cifs if c is not None]) if any(hard_cifs) else None\n",
    "    \n",
    "    finding = {\n",
    "        'model': model_key,\n",
    "        'cif_by_type': {et: analysis.get(et, {}).get('cif_rate', None) for et in ERROR_TYPE_NAMES},\n",
    "        'easy_cif': easy_cif,\n",
    "        'medium_cif': medium_cif,\n",
    "        'hard_cif': hard_cif,\n",
    "        'supports_hypothesis': easy_cif is not None and hard_cif is not None and hard_cif > easy_cif + 0.05\n",
    "    }\n",
    "    \n",
    "    if model_key in correlation_results:\n",
    "        finding['correlation'] = correlation_results[model_key]\n",
    "    \n",
    "    summary['key_findings'].append(finding)\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_EXP}/results/exp_A1_E4_summary.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('EXPERIMENT A1-E4 COMPLETE')\n",
    "print('='*60)\n",
    "print(f'\\nResults saved to: {SAVE_DIR_EXP}')\n",
    "print('\\n' + '='*60)\n",
    "print('KEY FINDINGS')\n",
    "print('='*60)\n",
    "\n",
    "for finding in summary['key_findings']:\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == finding['model']][0]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  CIF by error type:\")\n",
    "    for et, cif in finding['cif_by_type'].items():\n",
    "        if cif is not None:\n",
    "            detect = ERROR_TYPES[et]['detectability']\n",
    "            print(f\"    {et} ({detect}): {cif:.1%}\")\n",
    "    print(f\"  Easy avg: {finding['easy_cif']:.1%}\" if finding['easy_cif'] else \"  Easy avg: N/A\")\n",
    "    print(f\"  Hard avg: {finding['hard_cif']:.1%}\" if finding['hard_cif'] else \"  Hard avg: N/A\")\n",
    "    print(f\"  Supports hypothesis: {'✓ YES' if finding['supports_hypothesis'] else '? No'}\")\n",
    "    if 'correlation' in finding:\n",
    "        c = finding['correlation']\n",
    "        print(f\"  Correlation: r={c['correlation']:.3f}, p={c['p_value']:.4f}\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('INTERPRETATION')\n",
    "print('='*60)\n",
    "print('''\n",
    "If hypothesis supported (hard > easy CIF):\n",
    "  → Models can catch obvious arithmetic errors\n",
    "  → But struggle with conceptual/setup errors\n",
    "  → Implication: Error type matters for defense\n",
    "\n",
    "If not supported:\n",
    "  → CIF vulnerability uniform across error types\n",
    "  → Models either catch all or miss all\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
