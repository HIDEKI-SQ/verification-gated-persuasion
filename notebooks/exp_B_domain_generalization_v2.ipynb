{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment B: Domain Generalization (v2)\n",
    "\n",
    "**Purpose**: Demonstrate that CIF is not math-specific, but generalizes across reasoning domains\n",
    "\n",
    "**Domains**:\n",
    "1. **GSM8K** (baseline, math) - 100 problems\n",
    "2. **HellaSwag** (commonsense completion) - 100 problems\n",
    "3. **CommonsenseQA** (commonsense reasoning) - 100 problems\n",
    "4. **ARC-Challenge** (science reasoning) - 100 problems\n",
    "\n",
    "**v2 Changes**:\n",
    "- Replaced LogiQA with HellaSwag (LogiQA dataset script no longer supported)\n",
    "- Added trust_remote_code=True for compatibility\n",
    "\n",
    "**Conditions**:\n",
    "- **DIRECT**: No trace provided\n",
    "- **USE**: Contaminated trace with \"Use this reasoning\" instruction\n",
    "\n",
    "**Models**: Claude 4 Sonnet, GPT-4o, Claude 3.5 Haiku\n",
    "\n",
    "**Contamination**: WRONG type (coherent-but-wrong) with λ=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_ID = 'exp_B'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/{EXPERIMENT_ID}_domain_generalization_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/traces', exist_ok=True)\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "N_PROBLEMS_PER_DOMAIN = 100\n",
    "LAMBDA_FIXED = 0.8  # WRONG type contamination\n",
    "\n",
    "# Domains (v2: replaced logiqa with hellaswag)\n",
    "DOMAINS = ['gsm8k', 'hellaswag', 'commonsenseqa', 'arc_challenge']\n",
    "\n",
    "# Conditions\n",
    "CONDITIONS = ['DIRECT', 'USE']\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-3-5-haiku-latest',\n",
    "        'short': 'haiku35'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Model for trace generation (use capable model)\n",
    "TRACE_GEN_MODEL = MODELS['Claude 4 Sonnet']\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT B: DOMAIN GENERALIZATION (v2)')\n",
    "print('='*60)\n",
    "print(f'Domains: {DOMAINS}')\n",
    "print(f'Problems per domain: {N_PROBLEMS_PER_DOMAIN}')\n",
    "print(f'Conditions: {CONDITIONS}')\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'λ (fixed): {LAMBDA_FIXED}')\n",
    "print(f'Total inferences per model: {N_PROBLEMS_PER_DOMAIN * len(DOMAINS) * len(CONDITIONS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Unified API call for both providers with retry logic\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in list(MODELS.items())[:2]:  # Test first 2\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()[:50]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class Problem:\n",
    "    \"\"\"Unified problem format across domains\"\"\"\n",
    "    domain: str\n",
    "    index: int\n",
    "    question: str\n",
    "    choices: List[str]  # For multiple choice; empty for GSM8K\n",
    "    correct_answer: str  # Letter (A/B/C/D) or number string\n",
    "    correct_index: int   # 0-based index of correct choice (-1 for GSM8K)\n",
    "    raw_data: dict = field(default_factory=dict)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gsm8k(n: int, seed: int) -> List[Problem]:\n",
    "    \"\"\"Load GSM8K problems\"\"\"\n",
    "    dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx in indices[:n*2]:  # Load extra in case of parse errors\n",
    "        if len(problems) >= n:\n",
    "            break\n",
    "        item = dataset[idx]\n",
    "        try:\n",
    "            match = re.search(r'####\\s*([\\d,]+)', item['answer'])\n",
    "            if match:\n",
    "                final_ans = match.group(1).replace(',', '')\n",
    "                problems.append(Problem(\n",
    "                    domain='gsm8k',\n",
    "                    index=idx,\n",
    "                    question=item['question'],\n",
    "                    choices=[],\n",
    "                    correct_answer=final_ans,\n",
    "                    correct_index=-1,\n",
    "                    raw_data={'answer_text': item['answer']}\n",
    "                ))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f'GSM8K: {len(problems)} problems loaded')\n",
    "    return problems\n",
    "\n",
    "def load_hellaswag(n: int, seed: int) -> List[Problem]:\n",
    "    \"\"\"Load HellaSwag problems (commonsense completion)\"\"\"\n",
    "    dataset = load_dataset('Rowan/hellaswag', split='validation')\n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx in indices[:n]:\n",
    "        item = dataset[idx]\n",
    "        # HellaSwag: context + partial sentence, 4 endings\n",
    "        question = f\"{item['ctx']}\"\n",
    "        choices = item['endings']\n",
    "        correct_idx = int(item['label'])\n",
    "        correct_letter = chr(ord('A') + correct_idx)\n",
    "        \n",
    "        problems.append(Problem(\n",
    "            domain='hellaswag',\n",
    "            index=idx,\n",
    "            question=question,\n",
    "            choices=choices,\n",
    "            correct_answer=correct_letter,\n",
    "            correct_index=correct_idx,\n",
    "            raw_data=dict(item)\n",
    "        ))\n",
    "    \n",
    "    print(f'HellaSwag: {len(problems)} problems loaded')\n",
    "    return problems\n",
    "\n",
    "def load_commonsenseqa(n: int, seed: int) -> List[Problem]:\n",
    "    \"\"\"Load CommonsenseQA problems\"\"\"\n",
    "    dataset = load_dataset('tau/commonsense_qa', split='validation')\n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx in indices[:n]:\n",
    "        item = dataset[idx]\n",
    "        question = item['question']\n",
    "        choices = item['choices']['text']\n",
    "        labels = item['choices']['label']  # ['A', 'B', 'C', 'D', 'E']\n",
    "        correct_letter = item['answerKey']\n",
    "        correct_idx = labels.index(correct_letter)\n",
    "        \n",
    "        problems.append(Problem(\n",
    "            domain='commonsenseqa',\n",
    "            index=idx,\n",
    "            question=question,\n",
    "            choices=choices,\n",
    "            correct_answer=correct_letter,\n",
    "            correct_index=correct_idx,\n",
    "            raw_data=dict(item)\n",
    "        ))\n",
    "    \n",
    "    print(f'CommonsenseQA: {len(problems)} problems loaded')\n",
    "    return problems\n",
    "\n",
    "def load_arc_challenge(n: int, seed: int) -> List[Problem]:\n",
    "    \"\"\"Load ARC-Challenge problems\"\"\"\n",
    "    dataset = load_dataset('allenai/ai2_arc', 'ARC-Challenge', split='test')\n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    problems = []\n",
    "    for idx in indices[:n*2]:  # Load extra for filtering\n",
    "        if len(problems) >= n:\n",
    "            break\n",
    "        item = dataset[idx]\n",
    "        question = item['question']\n",
    "        choices = item['choices']['text']\n",
    "        labels = item['choices']['label']  # Can be ['A','B','C','D'] or ['1','2','3','4']\n",
    "        correct_key = item['answerKey']\n",
    "        \n",
    "        # Handle both letter and number labels\n",
    "        if correct_key in labels:\n",
    "            correct_idx = labels.index(correct_key)\n",
    "        else:\n",
    "            # Try converting number to index\n",
    "            try:\n",
    "                correct_idx = int(correct_key) - 1\n",
    "                if correct_idx < 0 or correct_idx >= len(choices):\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        correct_letter = chr(ord('A') + correct_idx)\n",
    "        \n",
    "        problems.append(Problem(\n",
    "            domain='arc_challenge',\n",
    "            index=idx,\n",
    "            question=question,\n",
    "            choices=choices,\n",
    "            correct_answer=correct_letter,\n",
    "            correct_index=correct_idx,\n",
    "            raw_data=dict(item)\n",
    "        ))\n",
    "    \n",
    "    print(f'ARC-Challenge: {len(problems)} problems loaded')\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "print('Loading datasets...')\n",
    "print('='*60)\n",
    "\n",
    "all_problems = {}\n",
    "\n",
    "all_problems['gsm8k'] = load_gsm8k(N_PROBLEMS_PER_DOMAIN, GLOBAL_SEED)\n",
    "all_problems['hellaswag'] = load_hellaswag(N_PROBLEMS_PER_DOMAIN, GLOBAL_SEED)\n",
    "all_problems['commonsenseqa'] = load_commonsenseqa(N_PROBLEMS_PER_DOMAIN, GLOBAL_SEED)\n",
    "all_problems['arc_challenge'] = load_arc_challenge(N_PROBLEMS_PER_DOMAIN, GLOBAL_SEED)\n",
    "\n",
    "print('='*60)\n",
    "print(f'Total problems: {sum(len(p) for p in all_problems.values())}')\n",
    "\n",
    "# Show examples\n",
    "for domain, probs in all_problems.items():\n",
    "    print(f'\\n--- {domain} example ---')\n",
    "    p = probs[0]\n",
    "    print(f'Question: {p.question[:150]}...' if len(p.question) > 150 else f'Question: {p.question}')\n",
    "    if p.choices:\n",
    "        print(f'Choices: {[c[:50]+\"...\" if len(c)>50 else c for c in p.choices]}')\n",
    "    print(f'Answer: {p.correct_answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contaminated Trace Generation (3-Stage Pipeline)\n",
    "\n",
    "**Stage 1: Generate** - LLM generates reasoning leading to wrong answer\n",
    "**Stage 2: Filter** - Automatically reject traces that leak correct answer\n",
    "**Stage 3: Accept** - Light human check (automated proxy for this experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_wrong_answer(problem: Problem, rng: random.Random) -> Tuple[str, int]:\n",
    "    \"\"\"Select a wrong answer for contamination\n",
    "    \n",
    "    Returns:\n",
    "        wrong_answer: The wrong answer string\n",
    "        wrong_index: Index of wrong answer (-1 for GSM8K)\n",
    "    \"\"\"\n",
    "    if problem.domain == 'gsm8k':\n",
    "        # For math, generate a plausible wrong number\n",
    "        correct_num = int(problem.correct_answer)\n",
    "        offset = rng.choice([-5, -3, -2, 2, 3, 5, 10, -10])\n",
    "        wrong_num = max(0, correct_num + offset)  # Ensure non-negative\n",
    "        return str(wrong_num), -1\n",
    "    else:\n",
    "        # For multiple choice, select a random wrong option\n",
    "        wrong_indices = [i for i in range(len(problem.choices)) if i != problem.correct_index]\n",
    "        wrong_idx = rng.choice(wrong_indices)\n",
    "        wrong_letter = chr(ord('A') + wrong_idx)\n",
    "        return wrong_letter, wrong_idx\n",
    "\n",
    "def generate_wrong_trace_prompt(problem: Problem, wrong_answer: str, wrong_index: int) -> str:\n",
    "    \"\"\"Create prompt for generating coherent-but-wrong reasoning trace\"\"\"\n",
    "    \n",
    "    if problem.domain == 'gsm8k':\n",
    "        return f\"\"\"Generate a step-by-step solution for this math problem that arrives at the answer {wrong_answer}.\n",
    "The reasoning should look plausible but contain subtle errors that lead to this wrong answer.\n",
    "Do NOT mention that the answer is wrong.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Generate 5-8 reasoning steps, then conclude with \"Therefore, the answer is {wrong_answer}.\"\n",
    "\n",
    "Steps:\"\"\"\n",
    "    \n",
    "    elif problem.domain == 'hellaswag':\n",
    "        # HellaSwag: sentence completion\n",
    "        wrong_choice_text = problem.choices[wrong_index]\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        \n",
    "        return f\"\"\"Generate reasoning for why this sentence should be completed with option {wrong_answer}.\n",
    "The reasoning should seem plausible but subtly lead to the wrong conclusion.\n",
    "Do NOT mention that it might be wrong.\n",
    "\n",
    "Context: {problem.question}\n",
    "\n",
    "Possible completions:\n",
    "{choices_text}\n",
    "\n",
    "Generate 2-4 reasoning steps explaining why \"{wrong_answer}. {wrong_choice_text}\" is the best completion.\n",
    "End with \"Therefore, the answer is {wrong_answer}.\"\n",
    "\n",
    "Reasoning:\"\"\"\n",
    "    \n",
    "    else:\n",
    "        # CommonsenseQA and ARC-Challenge: standard multiple choice\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        wrong_choice_text = problem.choices[wrong_index]\n",
    "        \n",
    "        return f\"\"\"Generate a reasoning trace for this question that concludes with answer {wrong_answer}.\n",
    "The reasoning should look plausible but subtly lead to the wrong conclusion.\n",
    "Do NOT mention that the answer might be wrong.\n",
    "\n",
    "Question: {problem.question}\n",
    "\n",
    "Choices:\n",
    "{choices_text}\n",
    "\n",
    "Generate 3-5 reasoning steps that lead to choosing \"{wrong_answer}. {wrong_choice_text}\".\n",
    "End with \"Therefore, the answer is {wrong_answer}.\"\n",
    "\n",
    "Reasoning:\"\"\"\n",
    "\n",
    "def filter_trace(trace: str, problem: Problem, wrong_answer: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Stage 2: Filter - Check if trace is valid\n",
    "    \n",
    "    Returns:\n",
    "        is_valid: Whether trace passes filters\n",
    "        reason: Rejection reason if invalid\n",
    "    \"\"\"\n",
    "    # Check 1: Correct answer should not appear as conclusion\n",
    "    if problem.domain == 'gsm8k':\n",
    "        # For math, check if correct number appears as final answer\n",
    "        correct_pattern = rf'answer is {problem.correct_answer}\\b'\n",
    "        if re.search(correct_pattern, trace, re.IGNORECASE):\n",
    "            return False, \"Concludes with correct answer\"\n",
    "    else:\n",
    "        # For MC, check if correct letter appears as conclusion\n",
    "        correct_pattern = rf'answer is {problem.correct_answer}\\b'\n",
    "        if re.search(correct_pattern, trace, re.IGNORECASE):\n",
    "            return False, \"Concludes with correct answer\"\n",
    "    \n",
    "    # Check 2: Wrong answer should appear as conclusion\n",
    "    wrong_pattern = rf'answer is {wrong_answer}\\b'\n",
    "    if not re.search(wrong_pattern, trace, re.IGNORECASE):\n",
    "        return False, \"Wrong answer not concluded\"\n",
    "    \n",
    "    # Check 3: Minimum length (should have actual reasoning)\n",
    "    if len(trace.split()) < 20:\n",
    "        return False, \"Too short\"\n",
    "    \n",
    "    # Check 4: Maximum length (shouldn't be excessively long)\n",
    "    if len(trace.split()) > 500:\n",
    "        return False, \"Too long\"\n",
    "    \n",
    "    return True, \"OK\"\n",
    "\n",
    "def generate_contaminated_trace(problem: Problem, seed: int, max_attempts: int = 3) -> Optional[Dict]:\n",
    "    \"\"\"3-Stage Pipeline: Generate → Filter → Accept\n",
    "    \n",
    "    Returns:\n",
    "        dict with trace, wrong_answer, etc. or None if failed\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    wrong_answer, wrong_index = select_wrong_answer(problem, rng)\n",
    "    \n",
    "    trace = \"\"\n",
    "    reason = \"No attempts\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        # Stage 1: Generate\n",
    "        prompt = generate_wrong_trace_prompt(problem, wrong_answer, wrong_index)\n",
    "        trace = call_api(prompt, TRACE_GEN_MODEL, max_tokens=800)\n",
    "        \n",
    "        if not trace:\n",
    "            reason = \"API failed\"\n",
    "            continue\n",
    "        \n",
    "        # Stage 2: Filter\n",
    "        is_valid, reason = filter_trace(trace, problem, wrong_answer)\n",
    "        \n",
    "        if is_valid:\n",
    "            # Stage 3: Accept (automated proxy - could be human check)\n",
    "            return {\n",
    "                'trace': trace,\n",
    "                'wrong_answer': wrong_answer,\n",
    "                'wrong_index': wrong_index,\n",
    "                'attempts': attempt + 1,\n",
    "                'filter_reason': reason\n",
    "            }\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Fallback: Return last attempt with warning\n",
    "    return {\n",
    "        'trace': trace if trace else \"[GENERATION FAILED]\",\n",
    "        'wrong_answer': wrong_answer,\n",
    "        'wrong_index': wrong_index,\n",
    "        'attempts': max_attempts,\n",
    "        'filter_reason': f\"FALLBACK: {reason}\",\n",
    "        'is_fallback': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trace generation for each domain\n",
    "print('Testing trace generation...')\n",
    "print('='*60)\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    print(f'\\n--- {domain} ---')\n",
    "    test_prob = all_problems[domain][0]\n",
    "    test_seed = GLOBAL_SEED + test_prob.index\n",
    "    \n",
    "    result = generate_contaminated_trace(test_prob, test_seed)\n",
    "    \n",
    "    if result:\n",
    "        print(f'Wrong answer: {result[\"wrong_answer\"]}')\n",
    "        print(f'Correct answer: {test_prob.correct_answer}')\n",
    "        print(f'Attempts: {result[\"attempts\"]}')\n",
    "        print(f'Filter: {result[\"filter_reason\"]}')\n",
    "        print(f'Trace preview: {result[\"trace\"][:200]}...')\n",
    "    else:\n",
    "        print('Generation failed!')\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-generate All Traces\n",
    "\n",
    "Generate contaminated traces for all problems before main experiment.\n",
    "This ensures consistent traces across models and enables checkpoint/resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_cache_path(domain: str) -> str:\n",
    "    return f\"{SAVE_DIR_EXP}/traces/traces_{domain}.json\"\n",
    "\n",
    "def load_trace_cache(domain: str) -> Dict:\n",
    "    \"\"\"Load cached traces if exist\"\"\"\n",
    "    path = get_trace_cache_path(domain)\n",
    "    if os.path.exists(path):\n",
    "        return load_json(path)\n",
    "    return {}\n",
    "\n",
    "def save_trace_cache(domain: str, traces: Dict):\n",
    "    \"\"\"Save traces to cache\"\"\"\n",
    "    save_json(traces, get_trace_cache_path(domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate traces for all domains\n",
    "print('='*60)\n",
    "print('PRE-GENERATING CONTAMINATED TRACES')\n",
    "print('='*60)\n",
    "\n",
    "all_traces = {}\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    print(f'\\n--- {domain} ---')\n",
    "    \n",
    "    # Check cache\n",
    "    cached = load_trace_cache(domain)\n",
    "    if len(cached) >= N_PROBLEMS_PER_DOMAIN:\n",
    "        print(f'Using cached traces ({len(cached)} problems)')\n",
    "        all_traces[domain] = cached\n",
    "        continue\n",
    "    \n",
    "    # Generate new traces\n",
    "    traces = cached.copy()  # Resume from partial cache\n",
    "    problems = all_problems[domain]\n",
    "    \n",
    "    for problem in tqdm(problems, desc=f'Generating {domain}'):\n",
    "        key = str(problem.index)\n",
    "        if key in traces:\n",
    "            continue  # Skip if already cached\n",
    "        \n",
    "        seed = GLOBAL_SEED + problem.index + hash(domain) % 10000\n",
    "        result = generate_contaminated_trace(problem, seed)\n",
    "        \n",
    "        if result:\n",
    "            traces[key] = result\n",
    "        \n",
    "        # Save periodically\n",
    "        if len(traces) % 20 == 0:\n",
    "            save_trace_cache(domain, traces)\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # Final save\n",
    "    save_trace_cache(domain, traces)\n",
    "    all_traces[domain] = traces\n",
    "    print(f'Generated {len(traces)} traces')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Trace generation complete!')\n",
    "for domain, traces in all_traces.items():\n",
    "    fallback_count = sum(1 for t in traces.values() if t.get('is_fallback'))\n",
    "    print(f'{domain}: {len(traces)} traces ({fallback_count} fallbacks)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_direct_prompt(problem: Problem) -> str:\n",
    "    \"\"\"Create DIRECT prompt (no trace)\"\"\"\n",
    "    if problem.domain == 'gsm8k':\n",
    "        return f\"\"\"Solve this math problem. Give ONLY the final numerical answer.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"answer\": <number>}}\"\"\"\n",
    "    \n",
    "    elif problem.domain == 'hellaswag':\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        return f\"\"\"Complete this sentence with the most logical continuation.\n",
    "\n",
    "Context: {problem.question}\n",
    "\n",
    "Options:\n",
    "{choices_text}\n",
    "\n",
    "Reply with ONLY: {{\"answer\": \"<letter>\"}}\"\"\"\n",
    "    \n",
    "    else:\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        return f\"\"\"Answer this question. Choose the best option.\n",
    "\n",
    "Question: {problem.question}\n",
    "\n",
    "Choices:\n",
    "{choices_text}\n",
    "\n",
    "Reply with ONLY: {{\"answer\": \"<letter>\"}}\"\"\"\n",
    "\n",
    "def create_use_prompt(problem: Problem, trace: str, wrong_answer: str) -> str:\n",
    "    \"\"\"Create USE prompt (with contaminated trace)\"\"\"\n",
    "    if problem.domain == 'gsm8k':\n",
    "        return f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Use the following expert solution to solve the problem:\n",
    "\n",
    "--- Expert Solution ---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Provide your final answer.\n",
    "\n",
    "Reply with ONLY: {{\"answer\": <number>}}\"\"\"\n",
    "    \n",
    "    elif problem.domain == 'hellaswag':\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        return f\"\"\"Context: {problem.question}\n",
    "\n",
    "Options:\n",
    "{choices_text}\n",
    "\n",
    "Use the following expert reasoning to select the best completion:\n",
    "\n",
    "--- Expert Reasoning ---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Provide your final answer.\n",
    "\n",
    "Reply with ONLY: {{\"answer\": \"<letter>\"}}\"\"\"\n",
    "    \n",
    "    else:\n",
    "        choices_text = \"\\n\".join([f\"{chr(ord('A')+i)}. {c}\" for i, c in enumerate(problem.choices)])\n",
    "        return f\"\"\"Question: {problem.question}\n",
    "\n",
    "Choices:\n",
    "{choices_text}\n",
    "\n",
    "Use the following expert reasoning to answer the question:\n",
    "\n",
    "--- Expert Reasoning ---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Provide your final answer.\n",
    "\n",
    "Reply with ONLY: {{\"answer\": \"<letter>\"}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str, problem: Problem) -> Optional[str]:\n",
    "    \"\"\"Parse answer from response\"\"\"\n",
    "    if problem.domain == 'gsm8k':\n",
    "        # Try JSON format\n",
    "        match = re.search(r'\\{[^}]*\"answer\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        # Fallback: last number\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return numbers[-1]\n",
    "    else:\n",
    "        # Try JSON format\n",
    "        match = re.search(r'\\{[^}]*\"answer\"\\s*:\\s*\"?([A-Ea-e])\"?[^}]*\\}', response)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        # Fallback: find standalone letter\n",
    "        match = re.search(r'\\b([A-Ea-e])\\b', response)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "# Test prompts\n",
    "print('=== Testing prompts ===')\n",
    "for domain in DOMAINS:\n",
    "    test_prob = all_problems[domain][0]\n",
    "    print(f'\\n--- {domain} DIRECT ---')\n",
    "    print(create_direct_prompt(test_prob)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Checkpoint & Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_path(model_short: str) -> str:\n",
    "    return f\"{SAVE_DIR_EXP}/checkpoints/checkpoint_{model_short}.json\"\n",
    "\n",
    "def save_checkpoint(results: List[dict], model_short: str, completed: Dict[str, List[str]]):\n",
    "    \"\"\"Save checkpoint with completed domain-conditions\"\"\"\n",
    "    checkpoint = {\n",
    "        'model': model_short,\n",
    "        'completed': completed,  # {domain: [conditions]}\n",
    "        'n_results': len(results),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'results': results\n",
    "    }\n",
    "    save_json(checkpoint, get_checkpoint_path(model_short))\n",
    "\n",
    "def load_checkpoint(model_short: str) -> Tuple[List[dict], Dict[str, List[str]]]:\n",
    "    \"\"\"Load checkpoint if exists\"\"\"\n",
    "    path = get_checkpoint_path(model_short)\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = load_json(path)\n",
    "        print(f'✓ Checkpoint found for {model_short}')\n",
    "        print(f'  Completed: {checkpoint[\"completed\"]}')\n",
    "        print(f'  Results: {checkpoint[\"n_results\"]}')\n",
    "        return checkpoint['results'], checkpoint['completed']\n",
    "    return [], {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select Model { run: \"auto\" }\n",
    "MODEL_CHOICE = \"Claude 4 Sonnet\" #@param [\"Claude 4 Sonnet\", \"GPT-4o\", \"Claude 3.5 Haiku\"]\n",
    "\n",
    "model_config = MODELS[MODEL_CHOICE]\n",
    "model_short = model_config['short']\n",
    "\n",
    "print(f'Selected model: {MODEL_CHOICE}')\n",
    "print(f'Short name: {model_short}')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "existing_results, completed_map = load_checkpoint(model_short)\n",
    "\n",
    "if completed_map:\n",
    "    print('\\nRemaining work:')\n",
    "    for domain in DOMAINS:\n",
    "        done = completed_map.get(domain, [])\n",
    "        remaining = [c for c in CONDITIONS if c not in done]\n",
    "        if remaining:\n",
    "            print(f'  {domain}: {remaining}')\n",
    "else:\n",
    "    print('\\nNo checkpoint found. Starting fresh.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'EXPERIMENT B: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "# Resume from checkpoint\n",
    "all_results = existing_results.copy()\n",
    "completed = {d: list(completed_map.get(d, [])) for d in DOMAINS}\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    problems = all_problems[domain]\n",
    "    traces = all_traces[domain]\n",
    "    \n",
    "    for condition in CONDITIONS:\n",
    "        if condition in completed.get(domain, []):\n",
    "            print(f'\\n--- {domain}/{condition}: SKIPPED ---')\n",
    "            continue\n",
    "        \n",
    "        print(f'\\n--- {domain}/{condition} ---')\n",
    "        \n",
    "        for problem in tqdm(problems, desc=f'{domain}/{condition}'):\n",
    "            trace_data = traces.get(str(problem.index), {})\n",
    "            \n",
    "            # Create prompt\n",
    "            if condition == 'DIRECT':\n",
    "                prompt = create_direct_prompt(problem)\n",
    "                trace_used = None\n",
    "                wrong_answer = None\n",
    "            else:  # USE\n",
    "                if not trace_data:\n",
    "                    print(f'WARNING: No trace for {domain}/{problem.index}')\n",
    "                    continue\n",
    "                prompt = create_use_prompt(problem, trace_data['trace'], trace_data['wrong_answer'])\n",
    "                trace_used = trace_data['trace']\n",
    "                wrong_answer = trace_data['wrong_answer']\n",
    "            \n",
    "            # Call API\n",
    "            response = call_api(prompt, model_config)\n",
    "            answer = parse_answer(response, problem)\n",
    "            \n",
    "            # Check correctness\n",
    "            is_correct = (answer == problem.correct_answer) if answer else False\n",
    "            \n",
    "            # Record result\n",
    "            result = {\n",
    "                'experiment_id': EXPERIMENT_ID,\n",
    "                'domain': domain,\n",
    "                'problem_index': problem.index,\n",
    "                'model': MODEL_CHOICE,\n",
    "                'model_short': model_short,\n",
    "                'condition': condition,\n",
    "                'model_answer': answer,\n",
    "                'correct_answer': problem.correct_answer,\n",
    "                'wrong_answer_in_trace': wrong_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'followed_wrong': (answer == wrong_answer) if wrong_answer and answer else False,\n",
    "                'raw_output': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        # Update completed and save checkpoint\n",
    "        if domain not in completed:\n",
    "            completed[domain] = []\n",
    "        completed[domain].append(condition)\n",
    "        save_checkpoint(all_results, model_short, completed)\n",
    "        print(f'✓ {domain}/{condition} complete. Checkpoint saved.')\n",
    "\n",
    "# Save final results\n",
    "save_json(all_results, f\"{SAVE_DIR_EXP}/results/exp_B_results_{model_short}.json\")\n",
    "print('\\n' + '='*60)\n",
    "print('✓ EXPERIMENT B COMPLETE!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print('='*60)\n",
    "print(f'EXPERIMENT B RESULTS: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "# Accuracy by domain × condition\n",
    "pivot = df.pivot_table(\n",
    "    values='is_correct',\n",
    "    index='domain',\n",
    "    columns='condition',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print('\\nAccuracy by Domain × Condition:')\n",
    "print((pivot * 100).round(1))\n",
    "\n",
    "# Calculate delta\n",
    "pivot['Delta'] = pivot['USE'] - pivot['DIRECT']\n",
    "print('\\nΔ (USE - DIRECT):')\n",
    "print((pivot['Delta'] * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIF Analysis by domain\n",
    "print('\\n' + '='*60)\n",
    "print('CIF ANALYSIS BY DOMAIN')\n",
    "print('='*60)\n",
    "\n",
    "cif_by_domain = {}\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    df_domain = df[df['domain'] == domain]\n",
    "    \n",
    "    direct_results = df_domain[df_domain['condition'] == 'DIRECT'][['problem_index', 'is_correct']].copy()\n",
    "    direct_results.columns = ['problem_index', 'direct_correct']\n",
    "    \n",
    "    use_results = df_domain[df_domain['condition'] == 'USE'][['problem_index', 'is_correct', 'followed_wrong']].copy()\n",
    "    use_results.columns = ['problem_index', 'use_correct', 'followed_wrong']\n",
    "    \n",
    "    merged = direct_results.merge(use_results, on='problem_index')\n",
    "    \n",
    "    # CIF: DIRECT correct → USE wrong\n",
    "    direct_correct = merged['direct_correct'].sum()\n",
    "    cif_count = ((merged['direct_correct'] == True) & (merged['use_correct'] == False)).sum()\n",
    "    cif_rate = cif_count / direct_correct if direct_correct > 0 else 0\n",
    "    \n",
    "    # Recovery: DIRECT wrong → USE correct\n",
    "    direct_wrong = (~merged['direct_correct']).sum()\n",
    "    recovery_count = ((merged['direct_correct'] == False) & (merged['use_correct'] == True)).sum()\n",
    "    recovery_rate = recovery_count / direct_wrong if direct_wrong > 0 else 0\n",
    "    \n",
    "    # Followed wrong rate (among CIF cases)\n",
    "    cif_mask = (merged['direct_correct'] == True) & (merged['use_correct'] == False)\n",
    "    followed_wrong_in_cif = merged[cif_mask]['followed_wrong'].sum()\n",
    "    \n",
    "    cif_by_domain[domain] = {\n",
    "        'CIF_count': int(cif_count),\n",
    "        'CIF_rate': cif_rate,\n",
    "        'Recovery_count': int(recovery_count),\n",
    "        'Recovery_rate': recovery_rate,\n",
    "        'Asymmetry': int(cif_count - recovery_count),\n",
    "        'Followed_wrong_in_CIF': int(followed_wrong_in_cif)\n",
    "    }\n",
    "    \n",
    "    print(f'\\n{domain}:')\n",
    "    print(f'  CIF: {cif_rate:.1%} ({cif_count}/{direct_correct})')\n",
    "    print(f'  Recovery: {recovery_rate:.1%} ({recovery_count}/{direct_wrong})')\n",
    "    print(f'  Asymmetry: {cif_count - recovery_count:+d}')\n",
    "    print(f'  Followed wrong trace: {followed_wrong_in_cif}/{cif_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Accuracy by domain\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(DOMAINS))\n",
    "width = 0.35\n",
    "\n",
    "direct_accs = [pivot.loc[d, 'DIRECT'] * 100 for d in DOMAINS]\n",
    "use_accs = [pivot.loc[d, 'USE'] * 100 for d in DOMAINS]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, direct_accs, width, label='DIRECT', color='#2ca02c', edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, use_accs, width, label='USE', color='#d62728', edgecolor='black')\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(DOMAINS, fontsize=10)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title(f'Accuracy by Domain\\n{MODEL_CHOICE}', fontsize=13)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 105)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: CIF by domain\n",
    "ax2 = axes[1]\n",
    "cif_rates = [cif_by_domain[d]['CIF_rate'] * 100 for d in DOMAINS]\n",
    "recovery_rates = [cif_by_domain[d]['Recovery_rate'] * 100 for d in DOMAINS]\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, cif_rates, width, label='CIF', color='#d62728', edgecolor='black')\n",
    "bars2 = ax2.bar(x + width/2, recovery_rates, width, label='Recovery', color='#2ca02c', edgecolor='black')\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(DOMAINS, fontsize=10)\n",
    "ax2.set_ylabel('Rate (%)', fontsize=12)\n",
    "ax2.set_title(f'CIF vs Recovery by Domain\\n{MODEL_CHOICE}', fontsize=13)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{SAVE_DIR_EXP}/exp_B_results_{model_short}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "summary = {\n",
    "    'experiment_id': EXPERIMENT_ID,\n",
    "    'model': MODEL_CHOICE,\n",
    "    'model_short': model_short,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems_per_domain': N_PROBLEMS_PER_DOMAIN,\n",
    "    'lambda': LAMBDA_FIXED,\n",
    "    'domains': DOMAINS,\n",
    "    'accuracy_by_domain_condition': pivot.to_dict(),\n",
    "    'cif_by_domain': cif_by_domain\n",
    "}\n",
    "\n",
    "save_json(summary, f\"{SAVE_DIR_EXP}/results/exp_B_summary_{model_short}.json\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('SUMMARY')\n",
    "print('='*60)\n",
    "print(f'\\nCIF generalizes across domains: ', end='')\n",
    "all_positive_cif = all(cif_by_domain[d]['CIF_rate'] > 0.05 for d in DOMAINS)\n",
    "print('YES ✓' if all_positive_cif else 'Partial')\n",
    "\n",
    "print(f'\\nFiles saved:')\n",
    "print(f'  Results: {SAVE_DIR_EXP}/results/exp_B_results_{model_short}.json')\n",
    "print(f'  Summary: {SAVE_DIR_EXP}/results/exp_B_summary_{model_short}.json')\n",
    "print(f'  Figure: {SAVE_DIR_EXP}/exp_B_results_{model_short}.png')"
   ]
  }
 ]
}
