{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT A1-E1: Format Swap Experiment\n",
    "\n",
    "## Purpose\n",
    "Demonstrate that **task format** (MC vs Open) determines CIF vulnerability, not domain.\n",
    "\n",
    "## Design\n",
    "| Original | Transform | Prediction |\n",
    "|----------|-----------|------------|\n",
    "| GSM8K (Open) | → MC (4択) | CIF ↑ |\n",
    "| CSQA (MC) | → Open | CIF ↓ |\n",
    "\n",
    "## Conditions\n",
    "- DIRECT: No trace\n",
    "- USE: Contaminated trace (λ=0.8)\n",
    "- USE_NOANS: Trace with answer removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: SETUP & DIRECTORIES\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_ID = 'A1_E1'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/exp_{EXPERIMENT_ID}_format_swap_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/traces', exist_ok=True)\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "!pip install datasets openai anthropic pandas tqdm matplotlib -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: IMPORTS & CONFIGURATION\n",
    "# ============================================================\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "GLOBAL_SEED = 20260120\n",
    "N_PROBLEMS = 100  # Per task\n",
    "LAMBDA_FIXED = 0.8\n",
    "\n",
    "# Conditions\n",
    "CONDITIONS = ['DIRECT', 'USE', 'USE_NOANS']\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Baseline from Experiment B (for comparison)\n",
    "EXP_B_BASELINE = {\n",
    "    'sonnet4': {\n",
    "        'gsm8k_open': {'direct': 0.96, 'use': 0.96, 'cif': 0.00},\n",
    "        'csqa_mc': {'direct': 0.90, 'use': 0.49, 'cif': 0.46}\n",
    "    },\n",
    "    'gpt4o': {\n",
    "        'gsm8k_open': {'direct': 0.58, 'use': 0.79, 'cif': 0.21},\n",
    "        'csqa_mc': {'direct': 0.85, 'use': 0.54, 'cif': 0.39}\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT A1-E1: FORMAT SWAP')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'λ (fixed): {LAMBDA_FIXED}')\n",
    "print(f'Conditions: {CONDITIONS}')\n",
    "print(f'Problems per task: {N_PROBLEMS}')\n",
    "print(f'Tasks: GSM8K-MC (converted), CSQA-Open (converted)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: UTILITY FUNCTIONS\n",
    "# ============================================================\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy/pandas types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    \"\"\"Save data to JSON with automatic type conversion\"\"\"\n",
    "    converted_data = convert_to_native(data)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"Load JSON file\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "print('Utility functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: API SETUP\n",
    "# ============================================================\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Unified API call for both providers with retry logic\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: LOAD DATASETS\n",
    "# ============================================================\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading datasets...')\n",
    "\n",
    "# Load GSM8K\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "print(f'✓ GSM8K loaded: {len(gsm8k_dataset)} problems')\n",
    "\n",
    "# Load CommonsenseQA\n",
    "csqa_dataset = load_dataset('tau/commonsense_qa', split='validation')\n",
    "print(f'✓ CommonsenseQA loaded: {len(csqa_dataset)} problems')\n",
    "\n",
    "# Sample with fixed seed\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "\n",
    "gsm8k_indices = list(range(len(gsm8k_dataset)))\n",
    "rng.shuffle(gsm8k_indices)\n",
    "gsm8k_indices = gsm8k_indices[:N_PROBLEMS]\n",
    "\n",
    "csqa_indices = list(range(len(csqa_dataset)))\n",
    "rng.shuffle(csqa_indices)\n",
    "csqa_indices = csqa_indices[:N_PROBLEMS]\n",
    "\n",
    "print(f'\\n✓ Sampled {N_PROBLEMS} problems from each dataset')\n",
    "print(f'GSM8K indices (first 5): {gsm8k_indices[:5]}')\n",
    "print(f'CSQA indices (first 5): {csqa_indices[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: HELPER FUNCTIONS FOR FORMAT CONVERSION\n",
    "# ============================================================\n",
    "\n",
    "def extract_gsm8k_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract numerical answer from GSM8K answer text.\"\"\"\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def generate_wrong_numbers(correct: str, n: int = 3) -> List[str]:\n",
    "    \"\"\"Generate plausible wrong numerical answers.\"\"\"\n",
    "    try:\n",
    "        correct_num = int(correct)\n",
    "        wrong_nums = set()\n",
    "        \n",
    "        candidates = [\n",
    "            correct_num + random.randint(1, 20),\n",
    "            correct_num - random.randint(1, 20),\n",
    "            correct_num * 2,\n",
    "            correct_num // 2 if correct_num > 1 else correct_num + 5,\n",
    "            correct_num + 10,\n",
    "            correct_num - 10,\n",
    "            int(correct_num * 1.5),\n",
    "            int(correct_num * 0.75),\n",
    "        ]\n",
    "        \n",
    "        for c in candidates:\n",
    "            if c > 0 and c != correct_num:\n",
    "                wrong_nums.add(str(c))\n",
    "            if len(wrong_nums) >= n:\n",
    "                break\n",
    "        \n",
    "        while len(wrong_nums) < n:\n",
    "            rand_num = correct_num + random.randint(-50, 50)\n",
    "            if rand_num > 0 and rand_num != correct_num:\n",
    "                wrong_nums.add(str(rand_num))\n",
    "        \n",
    "        return list(wrong_nums)[:n]\n",
    "    except:\n",
    "        return [str(random.randint(1, 100)) for _ in range(n)]\n",
    "\n",
    "def get_csqa_correct_text(problem: dict) -> str:\n",
    "    \"\"\"Get the correct answer text for CSQA problem.\"\"\"\n",
    "    correct_key = problem['answerKey']\n",
    "    choices = problem['choices']\n",
    "    for i, label in enumerate(choices['label']):\n",
    "        if label == correct_key:\n",
    "            return choices['text'][i]\n",
    "    return \"\"\n",
    "\n",
    "def get_csqa_wrong_text(problem: dict) -> Tuple[str, str]:\n",
    "    \"\"\"Get a random wrong answer text and its label for CSQA.\"\"\"\n",
    "    correct_key = problem['answerKey']\n",
    "    choices = problem['choices']\n",
    "    wrong_options = []\n",
    "    for i, label in enumerate(choices['label']):\n",
    "        if label != correct_key:\n",
    "            wrong_options.append((label, choices['text'][i]))\n",
    "    \n",
    "    if wrong_options:\n",
    "        return random.choice(wrong_options)\n",
    "    return (\"\", \"\")\n",
    "\n",
    "def remove_answer_from_trace(trace: str, answer: str) -> str:\n",
    "    \"\"\"Remove the final answer from trace.\"\"\"\n",
    "    patterns = [\n",
    "        rf'Therefore,?\\s*the answer is\\s*{re.escape(str(answer))}[.]?',\n",
    "        rf'answer is\\s*{re.escape(str(answer))}[.]?',\n",
    "        rf'The answer is\\s*{re.escape(str(answer))}[.]?',\n",
    "    ]\n",
    "    result = trace\n",
    "    for pattern in patterns:\n",
    "        result = re.sub(pattern, '[Answer removed]', result, flags=re.IGNORECASE)\n",
    "    return result\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: GENERATE GSM8K CONTAMINATED TRACES\n",
    "# ============================================================\n",
    "\n",
    "def generate_gsm8k_wrong_trace(question: str, correct_answer: str) -> Dict:\n",
    "    \"\"\"Generate a contaminated trace for GSM8K.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Solve this math problem step by step, but make a subtle error in your reasoning that leads to a wrong final answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Requirements:\n",
    "1. Show clear step-by-step reasoning\n",
    "2. Make ONE subtle mathematical error (wrong operation, miscalculation, or misreading)\n",
    "3. The error should seem plausible, not obvious\n",
    "4. End with \"Therefore, the answer is [NUMBER].\" where NUMBER is your (wrong) result\n",
    "5. Do NOT arrive at {correct_answer} - that's the correct answer\n",
    "\n",
    "Show your (intentionally flawed) solution:\"\"\"\n",
    "\n",
    "    # Use Sonnet for trace generation\n",
    "    trace = call_api(prompt, MODELS['Claude 4 Sonnet'], max_tokens=1500)\n",
    "    \n",
    "    # Extract wrong answer\n",
    "    match = re.search(r'answer is\\s*[\\$]?([\\d,]+)', trace, re.IGNORECASE)\n",
    "    wrong_answer = match.group(1).replace(',', '') if match else \"\"\n",
    "    \n",
    "    # Verify it's wrong\n",
    "    if wrong_answer == correct_answer:\n",
    "        try:\n",
    "            wrong_num = int(correct_answer) + random.choice([10, -10, 5, -5, 20])\n",
    "            if wrong_num < 0:\n",
    "                wrong_num = int(correct_answer) + 15\n",
    "            wrong_answer = str(wrong_num)\n",
    "            trace = re.sub(\n",
    "                r'answer is\\s*[\\$]?[\\d,]+',\n",
    "                f'answer is {wrong_answer}',\n",
    "                trace, flags=re.IGNORECASE\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'wrong_answer': wrong_answer,\n",
    "        'correct_answer': correct_answer\n",
    "    }\n",
    "\n",
    "# Generate or load traces\n",
    "trace_file = f'{SAVE_DIR_EXP}/traces/gsm8k_traces.json'\n",
    "gsm8k_traces = load_json(trace_file)\n",
    "\n",
    "if gsm8k_traces:\n",
    "    print(f'✓ Loaded {len(gsm8k_traces)} cached GSM8K traces')\n",
    "else:\n",
    "    gsm8k_traces = {}\n",
    "\n",
    "print(f'\\nGenerating GSM8K contaminated traces...')\n",
    "print(f'(Estimated time: ~{(N_PROBLEMS - len(gsm8k_traces)) * 0.1:.1f} minutes)')\n",
    "\n",
    "for idx in tqdm(gsm8k_indices, desc='GSM8K Traces'):\n",
    "    if str(idx) in gsm8k_traces:\n",
    "        continue\n",
    "    \n",
    "    problem = gsm8k_dataset[idx]\n",
    "    correct_answer = extract_gsm8k_answer(problem['answer'])\n",
    "    \n",
    "    trace_data = generate_gsm8k_wrong_trace(problem['question'], correct_answer)\n",
    "    gsm8k_traces[str(idx)] = trace_data\n",
    "    \n",
    "    # Save every 20\n",
    "    if len(gsm8k_traces) % 20 == 0:\n",
    "        save_json(gsm8k_traces, trace_file)\n",
    "\n",
    "save_json(gsm8k_traces, trace_file)\n",
    "print(f'\\n✓ GSM8K traces complete: {len(gsm8k_traces)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: CONVERT GSM8K TO MULTIPLE-CHOICE FORMAT\n",
    "# ============================================================\n",
    "\n",
    "def create_gsm8k_mc_problem(idx: int, problem: dict, trace_data: dict) -> Dict:\n",
    "    \"\"\"Convert GSM8K problem to multiple-choice format.\"\"\"\n",
    "    \n",
    "    question = problem['question']\n",
    "    correct_answer = extract_gsm8k_answer(problem['answer'])\n",
    "    wrong_from_trace = trace_data['wrong_answer']\n",
    "    \n",
    "    # Generate distractor answers\n",
    "    other_wrong = generate_wrong_numbers(correct_answer, 2)\n",
    "    \n",
    "    # Ensure uniqueness\n",
    "    if wrong_from_trace == correct_answer or wrong_from_trace in other_wrong:\n",
    "        wrong_from_trace = generate_wrong_numbers(correct_answer, 1)[0]\n",
    "    \n",
    "    # Create choices\n",
    "    choices = [\n",
    "        ('correct', correct_answer),\n",
    "        ('trace_wrong', wrong_from_trace),\n",
    "        ('distractor1', other_wrong[0]),\n",
    "        ('distractor2', other_wrong[1])\n",
    "    ]\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    formatted_choices = []\n",
    "    correct_label = None\n",
    "    trace_wrong_label = None\n",
    "    \n",
    "    for i, (choice_type, value) in enumerate(choices):\n",
    "        label = choice_labels[i]\n",
    "        formatted_choices.append(f\"{label}. {value}\")\n",
    "        if choice_type == 'correct':\n",
    "            correct_label = label\n",
    "        elif choice_type == 'trace_wrong':\n",
    "            trace_wrong_label = label\n",
    "    \n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'question': question,\n",
    "        'choices': formatted_choices,\n",
    "        'choices_text': \"\\n\".join(formatted_choices),\n",
    "        'correct_label': correct_label,\n",
    "        'correct_answer': correct_answer,\n",
    "        'trace_wrong_label': trace_wrong_label,\n",
    "        'trace_wrong_answer': wrong_from_trace,\n",
    "        'trace': trace_data['trace']\n",
    "    }\n",
    "\n",
    "# Convert\n",
    "print('Converting GSM8K to MC format...')\n",
    "gsm8k_mc_problems = []\n",
    "\n",
    "for idx in tqdm(gsm8k_indices, desc='GSM8K→MC'):\n",
    "    problem = gsm8k_dataset[idx]\n",
    "    trace_data = gsm8k_traces[str(idx)]\n",
    "    mc_problem = create_gsm8k_mc_problem(idx, problem, trace_data)\n",
    "    gsm8k_mc_problems.append(mc_problem)\n",
    "\n",
    "print(f'\\n✓ Created {len(gsm8k_mc_problems)} GSM8K-MC problems')\n",
    "\n",
    "# Show example\n",
    "ex = gsm8k_mc_problems[0]\n",
    "print('\\n' + '='*50)\n",
    "print('EXAMPLE GSM8K-MC:')\n",
    "print('='*50)\n",
    "print(f\"Q: {ex['question'][:150]}...\")\n",
    "print(f\"\\n{ex['choices_text']}\")\n",
    "print(f\"\\nCorrect: {ex['correct_label']} ({ex['correct_answer']})\")\n",
    "print(f\"Trace wrong: {ex['trace_wrong_label']} ({ex['trace_wrong_answer']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: GENERATE CSQA CONTAMINATED TRACES\n",
    "# ============================================================\n",
    "\n",
    "def generate_csqa_wrong_trace(question: str, correct_text: str, wrong_text: str) -> Dict:\n",
    "    \"\"\"Generate a contaminated trace for CSQA.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Answer this common sense question, but arrive at a specific (wrong) answer through plausible-sounding reasoning.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "You MUST conclude that the answer is: {wrong_text}\n",
    "\n",
    "Requirements:\n",
    "1. Provide step-by-step reasoning that sounds logical\n",
    "2. Make it seem like \"{wrong_text}\" is the sensible answer\n",
    "3. End with \"Therefore, the answer is {wrong_text}.\"\n",
    "4. Do NOT mention that this reasoning is flawed\n",
    "\n",
    "Show your reasoning:\"\"\"\n",
    "\n",
    "    trace = call_api(prompt, MODELS['Claude 4 Sonnet'], max_tokens=1000)\n",
    "    \n",
    "    if wrong_text.lower() not in trace.lower():\n",
    "        trace += f\"\\n\\nTherefore, the answer is {wrong_text}.\"\n",
    "    \n",
    "    return {\n",
    "        'trace': trace,\n",
    "        'wrong_answer': wrong_text,\n",
    "        'correct_answer': correct_text\n",
    "    }\n",
    "\n",
    "# Generate or load traces\n",
    "csqa_trace_file = f'{SAVE_DIR_EXP}/traces/csqa_traces.json'\n",
    "csqa_traces = load_json(csqa_trace_file)\n",
    "\n",
    "if csqa_traces:\n",
    "    print(f'✓ Loaded {len(csqa_traces)} cached CSQA traces')\n",
    "else:\n",
    "    csqa_traces = {}\n",
    "\n",
    "print(f'\\nGenerating CSQA contaminated traces...')\n",
    "print(f'(Estimated time: ~{(N_PROBLEMS - len(csqa_traces)) * 0.1:.1f} minutes)')\n",
    "\n",
    "for idx in tqdm(csqa_indices, desc='CSQA Traces'):\n",
    "    if str(idx) in csqa_traces:\n",
    "        continue\n",
    "    \n",
    "    problem = csqa_dataset[idx]\n",
    "    correct_text = get_csqa_correct_text(problem)\n",
    "    wrong_label, wrong_text = get_csqa_wrong_text(problem)\n",
    "    \n",
    "    trace_data = generate_csqa_wrong_trace(problem['question'], correct_text, wrong_text)\n",
    "    trace_data['wrong_label'] = wrong_label\n",
    "    csqa_traces[str(idx)] = trace_data\n",
    "    \n",
    "    if len(csqa_traces) % 20 == 0:\n",
    "        save_json(csqa_traces, csqa_trace_file)\n",
    "\n",
    "save_json(csqa_traces, csqa_trace_file)\n",
    "print(f'\\n✓ CSQA traces complete: {len(csqa_traces)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: PREPARE CSQA OPEN-ENDED PROBLEMS\n",
    "# ============================================================\n",
    "\n",
    "def create_csqa_open_problem(idx: int, problem: dict, trace_data: dict) -> Dict:\n",
    "    \"\"\"Create CSQA problem in open-ended format.\"\"\"\n",
    "    \n",
    "    question = problem['question']\n",
    "    correct_text = get_csqa_correct_text(problem)\n",
    "    \n",
    "    all_choices = {label: text for label, text in \n",
    "                   zip(problem['choices']['label'], problem['choices']['text'])}\n",
    "    \n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'question': question,\n",
    "        'correct_answer': correct_text,\n",
    "        'correct_label': problem['answerKey'],\n",
    "        'wrong_answer': trace_data['wrong_answer'],\n",
    "        'wrong_label': trace_data.get('wrong_label', ''),\n",
    "        'all_choices': all_choices,\n",
    "        'trace': trace_data['trace']\n",
    "    }\n",
    "\n",
    "print('Creating CSQA open-ended problems...')\n",
    "csqa_open_problems = []\n",
    "\n",
    "for idx in tqdm(csqa_indices, desc='CSQA→Open'):\n",
    "    problem = csqa_dataset[idx]\n",
    "    trace_data = csqa_traces[str(idx)]\n",
    "    open_problem = create_csqa_open_problem(idx, problem, trace_data)\n",
    "    csqa_open_problems.append(open_problem)\n",
    "\n",
    "print(f'\\n✓ Created {len(csqa_open_problems)} CSQA-Open problems')\n",
    "\n",
    "# Show example\n",
    "ex = csqa_open_problems[0]\n",
    "print('\\n' + '='*50)\n",
    "print('EXAMPLE CSQA-OPEN:')\n",
    "print('='*50)\n",
    "print(f\"Q: {ex['question']}\")\n",
    "print(f\"Correct: {ex['correct_answer']}\")\n",
    "print(f\"Trace wrong: {ex['wrong_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: PROMPT TEMPLATES\n",
    "# ============================================================\n",
    "\n",
    "# GSM8K-MC Prompts\n",
    "GSM8K_MC_PROMPTS = {\n",
    "    'DIRECT': \"\"\"Solve this math problem and select the correct answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "{choices}\n",
    "\n",
    "Select one answer (A, B, C, or D). Respond with ONLY the letter.\"\"\",\n",
    "\n",
    "    'USE': \"\"\"Solve this math problem and select the correct answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "{choices}\n",
    "\n",
    "An expert provided this solution approach:\n",
    "---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Use the expert's reasoning to help you select the correct answer.\n",
    "Select one answer (A, B, C, or D). Respond with ONLY the letter.\"\"\",\n",
    "\n",
    "    'USE_NOANS': \"\"\"Solve this math problem and select the correct answer.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "{choices}\n",
    "\n",
    "An expert provided this solution approach:\n",
    "---\n",
    "{trace_noans}\n",
    "---\n",
    "\n",
    "Use the expert's reasoning to help you select the correct answer.\n",
    "Select one answer (A, B, C, or D). Respond with ONLY the letter.\"\"\"\n",
    "}\n",
    "\n",
    "# CSQA-Open Prompts\n",
    "CSQA_OPEN_PROMPTS = {\n",
    "    'DIRECT': \"\"\"Answer this question with a single word or short phrase.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide ONLY your answer (a single word or short phrase).\"\"\",\n",
    "\n",
    "    'USE': \"\"\"Answer this question with a single word or short phrase.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "An expert provided this reasoning:\n",
    "---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "Use the expert's reasoning to help you answer.\n",
    "Provide ONLY your answer (a single word or short phrase).\"\"\",\n",
    "\n",
    "    'USE_NOANS': \"\"\"Answer this question with a single word or short phrase.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "An expert provided this reasoning:\n",
    "---\n",
    "{trace_noans}\n",
    "---\n",
    "\n",
    "Use the expert's reasoning to help you answer.\n",
    "Provide ONLY your answer (a single word or short phrase).\"\"\"\n",
    "}\n",
    "\n",
    "print('Prompt templates defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: RUN GSM8K-MC EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "def extract_mc_answer(response: str) -> str:\n",
    "    \"\"\"Extract letter answer from response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    match = re.search(r'^([A-D])', response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    match = re.search(r'\\b([A-D])\\b', response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def run_gsm8k_mc_experiment(model_name: str, model_config: dict) -> Dict:\n",
    "    \"\"\"Run GSM8K-MC experiment for a single model.\"\"\"\n",
    "    \n",
    "    short_name = model_config['short']\n",
    "    checkpoint_file = f'{SAVE_DIR_EXP}/checkpoints/gsm8k_mc_{short_name}.json'\n",
    "    \n",
    "    results = load_json(checkpoint_file)\n",
    "    if results:\n",
    "        print(f'✓ Loaded checkpoint: {len(results[\"problems\"])} problems')\n",
    "    else:\n",
    "        results = {'model': model_name, 'task': 'gsm8k_mc', 'problems': []}\n",
    "    \n",
    "    completed_indices = {p['idx'] for p in results['problems']}\n",
    "    \n",
    "    for problem in tqdm(gsm8k_mc_problems, desc=f'GSM8K-MC {short_name}'):\n",
    "        if problem['idx'] in completed_indices:\n",
    "            continue\n",
    "        \n",
    "        trace_noans = remove_answer_from_trace(\n",
    "            problem['trace'], problem['trace_wrong_answer']\n",
    "        )\n",
    "        \n",
    "        problem_result = {\n",
    "            'idx': problem['idx'],\n",
    "            'correct_label': problem['correct_label'],\n",
    "            'trace_wrong_label': problem['trace_wrong_label'],\n",
    "            'responses': {}\n",
    "        }\n",
    "        \n",
    "        for condition in CONDITIONS:\n",
    "            prompt = GSM8K_MC_PROMPTS[condition].format(\n",
    "                question=problem['question'],\n",
    "                choices=problem['choices_text'],\n",
    "                trace=problem['trace'],\n",
    "                trace_noans=trace_noans\n",
    "            )\n",
    "            \n",
    "            response = call_api(prompt, model_config, max_tokens=50)\n",
    "            answer = extract_mc_answer(response)\n",
    "            \n",
    "            problem_result['responses'][condition] = {\n",
    "                'raw': response,\n",
    "                'answer': answer,\n",
    "                'correct': answer == problem['correct_label'],\n",
    "                'followed_wrong': answer == problem['trace_wrong_label']\n",
    "            }\n",
    "        \n",
    "        results['problems'].append(problem_result)\n",
    "        \n",
    "        if len(results['problems']) % 20 == 0:\n",
    "            save_json(results, checkpoint_file)\n",
    "    \n",
    "    save_json(results, checkpoint_file)\n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "print('\\n' + '='*60)\n",
    "print('RUNNING GSM8K-MC EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "gsm8k_mc_results = {}\n",
    "for model_name, model_config in MODELS.items():\n",
    "    print(f'\\n--- {model_name} ---')\n",
    "    gsm8k_mc_results[model_config['short']] = run_gsm8k_mc_experiment(model_name, model_config)\n",
    "\n",
    "print('\\n✓ GSM8K-MC experiment complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: RUN CSQA-OPEN EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    return text.lower().strip().rstrip('.').rstrip('!')\n",
    "\n",
    "def check_csqa_open_answer(response: str, problem: Dict) -> Tuple[bool, bool]:\n",
    "    \"\"\"Check if response matches correct or wrong answer.\"\"\"\n",
    "    response_norm = normalize_text(response)\n",
    "    correct_norm = normalize_text(problem['correct_answer'])\n",
    "    wrong_norm = normalize_text(problem['wrong_answer'])\n",
    "    \n",
    "    is_correct = (\n",
    "        correct_norm in response_norm or \n",
    "        response_norm in correct_norm or\n",
    "        response_norm == correct_norm\n",
    "    )\n",
    "    \n",
    "    followed_wrong = (\n",
    "        wrong_norm in response_norm or \n",
    "        response_norm in wrong_norm or\n",
    "        response_norm == wrong_norm\n",
    "    )\n",
    "    \n",
    "    return is_correct, followed_wrong\n",
    "\n",
    "def run_csqa_open_experiment(model_name: str, model_config: dict) -> Dict:\n",
    "    \"\"\"Run CSQA-Open experiment for a single model.\"\"\"\n",
    "    \n",
    "    short_name = model_config['short']\n",
    "    checkpoint_file = f'{SAVE_DIR_EXP}/checkpoints/csqa_open_{short_name}.json'\n",
    "    \n",
    "    results = load_json(checkpoint_file)\n",
    "    if results:\n",
    "        print(f'✓ Loaded checkpoint: {len(results[\"problems\"])} problems')\n",
    "    else:\n",
    "        results = {'model': model_name, 'task': 'csqa_open', 'problems': []}\n",
    "    \n",
    "    completed_indices = {p['idx'] for p in results['problems']}\n",
    "    \n",
    "    for problem in tqdm(csqa_open_problems, desc=f'CSQA-Open {short_name}'):\n",
    "        if problem['idx'] in completed_indices:\n",
    "            continue\n",
    "        \n",
    "        trace_noans = remove_answer_from_trace(\n",
    "            problem['trace'], problem['wrong_answer']\n",
    "        )\n",
    "        \n",
    "        problem_result = {\n",
    "            'idx': problem['idx'],\n",
    "            'correct_answer': problem['correct_answer'],\n",
    "            'wrong_answer': problem['wrong_answer'],\n",
    "            'responses': {}\n",
    "        }\n",
    "        \n",
    "        for condition in CONDITIONS:\n",
    "            prompt = CSQA_OPEN_PROMPTS[condition].format(\n",
    "                question=problem['question'],\n",
    "                trace=problem['trace'],\n",
    "                trace_noans=trace_noans\n",
    "            )\n",
    "            \n",
    "            response = call_api(prompt, model_config, max_tokens=50)\n",
    "            is_correct, followed_wrong = check_csqa_open_answer(response, problem)\n",
    "            \n",
    "            problem_result['responses'][condition] = {\n",
    "                'raw': response,\n",
    "                'correct': is_correct,\n",
    "                'followed_wrong': followed_wrong\n",
    "            }\n",
    "        \n",
    "        results['problems'].append(problem_result)\n",
    "        \n",
    "        if len(results['problems']) % 20 == 0:\n",
    "            save_json(results, checkpoint_file)\n",
    "    \n",
    "    save_json(results, checkpoint_file)\n",
    "    return results\n",
    "\n",
    "# Run experiment\n",
    "print('\\n' + '='*60)\n",
    "print('RUNNING CSQA-OPEN EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "csqa_open_results = {}\n",
    "for model_name, model_config in MODELS.items():\n",
    "    print(f'\\n--- {model_name} ---')\n",
    "    csqa_open_results[model_config['short']] = run_csqa_open_experiment(model_name, model_config)\n",
    "\n",
    "print('\\n✓ CSQA-Open experiment complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: ANALYZE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "def analyze_task_results(results: Dict) -> Dict:\n",
    "    \"\"\"Analyze experiment results.\"\"\"\n",
    "    n = len(results['problems'])\n",
    "    \n",
    "    analysis = {\n",
    "        'n_problems': n,\n",
    "        'accuracy': {},\n",
    "        'cif_rate': {},\n",
    "        'followed_wrong_in_cif': {}\n",
    "    }\n",
    "    \n",
    "    for cond in CONDITIONS:\n",
    "        correct = sum(1 for p in results['problems'] if p['responses'][cond]['correct'])\n",
    "        analysis['accuracy'][cond] = correct / n if n > 0 else 0\n",
    "        \n",
    "        if cond != 'DIRECT':\n",
    "            direct_correct = [p for p in results['problems'] if p['responses']['DIRECT']['correct']]\n",
    "            cif_cases = [p for p in direct_correct if not p['responses'][cond]['correct']]\n",
    "            \n",
    "            analysis['cif_rate'][cond] = len(cif_cases) / len(direct_correct) if direct_correct else 0\n",
    "            \n",
    "            followed = sum(1 for p in cif_cases if p['responses'][cond].get('followed_wrong', False))\n",
    "            analysis['followed_wrong_in_cif'][cond] = followed / len(cif_cases) if cif_cases else 0\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze all\n",
    "print('\\n' + '='*60)\n",
    "print('EXPERIMENT A1-E1 RESULTS')\n",
    "print('='*60)\n",
    "\n",
    "all_analyses = {}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'{model_name}')\n",
    "    print('='*60)\n",
    "    \n",
    "    # GSM8K-MC\n",
    "    gsm_analysis = analyze_task_results(gsm8k_mc_results[model_key])\n",
    "    print(f'\\n--- GSM8K-MC (Converted from Open) ---')\n",
    "    for cond in CONDITIONS:\n",
    "        print(f\"  {cond}: {gsm_analysis['accuracy'][cond]:.1%}\")\n",
    "    print(f\"  CIF (USE): {gsm_analysis['cif_rate'].get('USE', 0):.1%}\")\n",
    "    print(f\"  Followed Wrong: {gsm_analysis['followed_wrong_in_cif'].get('USE', 0):.1%}\")\n",
    "    \n",
    "    # CSQA-Open\n",
    "    csqa_analysis = analyze_task_results(csqa_open_results[model_key])\n",
    "    print(f'\\n--- CSQA-Open (Converted from MC) ---')\n",
    "    for cond in CONDITIONS:\n",
    "        print(f\"  {cond}: {csqa_analysis['accuracy'][cond]:.1%}\")\n",
    "    print(f\"  CIF (USE): {csqa_analysis['cif_rate'].get('USE', 0):.1%}\")\n",
    "    print(f\"  Followed Wrong: {csqa_analysis['followed_wrong_in_cif'].get('USE', 0):.1%}\")\n",
    "    \n",
    "    all_analyses[model_key] = {\n",
    "        'gsm8k_mc': gsm_analysis,\n",
    "        'csqa_open': csqa_analysis\n",
    "    }\n",
    "\n",
    "save_json(all_analyses, f'{SAVE_DIR_EXP}/results/exp_A1_E1_analysis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: COMPARISON WITH EXP B BASELINE\n",
    "# ============================================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('FORMAT SWAP EFFECT COMPARISON')\n",
    "print('='*60)\n",
    "print('\\nQuestion: Does FORMAT cause CIF, not domain?')\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == model_key][0]\n",
    "    print(f'\\n{model_name}')\n",
    "    print('-'*50)\n",
    "    \n",
    "    baseline = EXP_B_BASELINE.get(model_key, {})\n",
    "    new = all_analyses[model_key]\n",
    "    \n",
    "    # GSM8K: Open → MC\n",
    "    old_cif = baseline.get('gsm8k_open', {}).get('cif', 0)\n",
    "    new_cif = new['gsm8k_mc']['cif_rate'].get('USE', 0)\n",
    "    delta_gsm = new_cif - old_cif\n",
    "    \n",
    "    print(f'\\nGSM8K (Open → MC):')\n",
    "    print(f'  Original (Open): CIF = {old_cif:.1%}')\n",
    "    print(f'  Converted (MC):  CIF = {new_cif:.1%}')\n",
    "    print(f'  → Format effect: {delta_gsm:+.1%}')\n",
    "    \n",
    "    # CSQA: MC → Open\n",
    "    old_cif = baseline.get('csqa_mc', {}).get('cif', 0)\n",
    "    new_cif = new['csqa_open']['cif_rate'].get('USE', 0)\n",
    "    delta_csqa = new_cif - old_cif\n",
    "    \n",
    "    print(f'\\nCSQA (MC → Open):')\n",
    "    print(f'  Original (MC):   CIF = {old_cif:.1%}')\n",
    "    print(f'  Converted (Open): CIF = {new_cif:.1%}')\n",
    "    print(f'  → Format effect: {delta_csqa:+.1%}')\n",
    "    \n",
    "    # Interpretation\n",
    "    supports = delta_gsm > 0.05 and delta_csqa < -0.05\n",
    "    print(f'\\n  Supports hypothesis: {\"✓ YES\" if supports else \"? Partial/No\"}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('INTERPRETATION')\n",
    "print('='*60)\n",
    "print('''\n",
    "If GSM8K-MC shows HIGHER CIF than GSM8K-Open,\n",
    "AND CSQA-Open shows LOWER CIF than CSQA-MC,\n",
    "→ FORMAT (not domain) determines CIF vulnerability.\n",
    "\n",
    "Causal mechanism:\n",
    "- MC format: Answer directly selectable → high CIF\n",
    "- Open format: Must generate answer → low CIF\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: VISUALIZATION\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = {'sonnet4': '#5B8FF9', 'gpt4o': '#5AD8A6'}\n",
    "model_labels = {'sonnet4': 'Claude 4 Sonnet', 'gpt4o': 'GPT-4o'}\n",
    "\n",
    "# Plot 1: CIF Rate Comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    baseline = EXP_B_BASELINE.get(model_key, {})\n",
    "    new = all_analyses[model_key]\n",
    "    \n",
    "    # Original format\n",
    "    original = [baseline.get('gsm8k_open', {}).get('cif', 0),\n",
    "                baseline.get('csqa_mc', {}).get('cif', 0)]\n",
    "    # Swapped format\n",
    "    swapped = [new['gsm8k_mc']['cif_rate'].get('USE', 0),\n",
    "               new['csqa_open']['cif_rate'].get('USE', 0)]\n",
    "    \n",
    "    offset = (i - 0.5) * width\n",
    "    ax1.bar(x + offset - width/4, original, width/2, \n",
    "            label=f'{model_labels[model_key]} (Original)', \n",
    "            color=colors[model_key], alpha=0.4)\n",
    "    ax1.bar(x + offset + width/4, swapped, width/2,\n",
    "            label=f'{model_labels[model_key]} (Swapped)', \n",
    "            color=colors[model_key], alpha=1.0)\n",
    "\n",
    "ax1.set_ylabel('CIF Rate', fontsize=12)\n",
    "ax1.set_title('Format Swap Effect on CIF', fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(['GSM8K\\n(Open→MC)', 'CSQA\\n(MC→Open)'])\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy by condition\n",
    "ax2 = axes[1]\n",
    "\n",
    "for i, model_key in enumerate(['sonnet4', 'gpt4o']):\n",
    "    new = all_analyses[model_key]\n",
    "    \n",
    "    direct = [new['gsm8k_mc']['accuracy']['DIRECT'],\n",
    "              new['csqa_open']['accuracy']['DIRECT']]\n",
    "    use = [new['gsm8k_mc']['accuracy']['USE'],\n",
    "           new['csqa_open']['accuracy']['USE']]\n",
    "    \n",
    "    offset = (i - 0.5) * width\n",
    "    ax2.bar(x + offset - width/4, direct, width/2,\n",
    "            label=f'{model_labels[model_key]} (DIRECT)', \n",
    "            color=colors[model_key], alpha=0.4)\n",
    "    ax2.bar(x + offset + width/4, use, width/2,\n",
    "            label=f'{model_labels[model_key]} (USE)', \n",
    "            color=colors[model_key], alpha=1.0)\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy: Swapped Formats', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['GSM8K-MC', 'CSQA-Open'])\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_EXP}/exp_A1_E1_format_swap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n✓ Figure saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "summary = {\n",
    "    'experiment_id': 'A1_E1',\n",
    "    'experiment_name': 'Format Swap',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'hypothesis': 'Task format (MC vs Open) determines CIF vulnerability, not domain',\n",
    "    'design': {\n",
    "        'gsm8k_mc': 'GSM8K converted from open-ended to 4-choice MC',\n",
    "        'csqa_open': 'CSQA converted from MC to open-ended'\n",
    "    },\n",
    "    'n_problems': N_PROBLEMS,\n",
    "    'lambda': LAMBDA_FIXED,\n",
    "    'models': list(MODELS.keys()),\n",
    "    'conditions': CONDITIONS,\n",
    "    'results': all_analyses,\n",
    "    'baseline_comparison': EXP_B_BASELINE,\n",
    "    'key_findings': []\n",
    "}\n",
    "\n",
    "for model_key in ['sonnet4', 'gpt4o']:\n",
    "    baseline = EXP_B_BASELINE.get(model_key, {})\n",
    "    new = all_analyses[model_key]\n",
    "    \n",
    "    gsm_delta = new['gsm8k_mc']['cif_rate'].get('USE', 0) - baseline.get('gsm8k_open', {}).get('cif', 0)\n",
    "    csqa_delta = new['csqa_open']['cif_rate'].get('USE', 0) - baseline.get('csqa_mc', {}).get('cif', 0)\n",
    "    \n",
    "    summary['key_findings'].append({\n",
    "        'model': model_key,\n",
    "        'gsm8k_format_effect': gsm_delta,\n",
    "        'csqa_format_effect': csqa_delta,\n",
    "        'supports_hypothesis': gsm_delta > 0.05 and csqa_delta < -0.05\n",
    "    })\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_EXP}/results/exp_A1_E1_summary.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('EXPERIMENT A1-E1 COMPLETE')\n",
    "print('='*60)\n",
    "print(f'\\nResults saved to: {SAVE_DIR_EXP}')\n",
    "print('\\nKey Files:')\n",
    "print('  - results/exp_A1_E1_summary.json')\n",
    "print('  - results/exp_A1_E1_analysis.json')\n",
    "print('  - exp_A1_E1_format_swap.png')\n",
    "print('\\n' + '='*60)\n",
    "print('KEY FINDINGS')\n",
    "print('='*60)\n",
    "\n",
    "for finding in summary['key_findings']:\n",
    "    model_name = [n for n, c in MODELS.items() if c['short'] == finding['model']][0]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  GSM8K: Open→MC = {finding['gsm8k_format_effect']:+.1%} CIF\")\n",
    "    print(f\"  CSQA:  MC→Open = {finding['csqa_format_effect']:+.1%} CIF\")\n",
    "    print(f\"  Supports: {'✓ YES' if finding['supports_hypothesis'] else '? Partial'}\")\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
