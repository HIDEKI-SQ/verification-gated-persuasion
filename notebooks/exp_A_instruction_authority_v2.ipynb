{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment A: Instruction × Authority × Answer Removal (v2)\n",
    "\n",
    "**Purpose**: Resolve Study 1-3 contradiction and decompose CIF firing conditions\n",
    "\n",
    "**v2 Changes**:\n",
    "- Fixed numpy int64/float64 JSON serialization issue\n",
    "- Improved error handling throughout\n",
    "- Better checkpoint resume logic\n",
    "\n",
    "**Design (2×2×2 - 2 = 6 conditions)**:\n",
    "- **Instruction**: USE vs NEUTRAL\n",
    "- **Authority**: AUTHORITATIVE vs UNCERTAIN\n",
    "- **Answer**: Present vs Removed\n",
    "\n",
    "| ID | Instruction | Authority | Answer | Description |\n",
    "|----|-------------|-----------|--------|-------------|\n",
    "| A1 | USE | AUTH | Present | \"Use the following expert solution\" |\n",
    "| A2 | USE | UNCERT | Present | \"Use the following attempted solution (may contain errors)\" |\n",
    "| A3 | NEUTRAL | AUTH | Present | \"Here is an expert reasoning trace\" |\n",
    "| A4 | NEUTRAL | UNCERT | Present | \"Here is an attempted reasoning trace (may contain errors)\" |\n",
    "| A5 | USE | AUTH | **Removed** | A1 without final answer |\n",
    "| A6 | USE | UNCERT | **Removed** | A2 without final answer |\n",
    "| A0 | — | — | — | DIRECT baseline (no trace) |\n",
    "\n",
    "**Models**: Claude 4 Sonnet, GPT-4o, Claude 3.5 Haiku\n",
    "\n",
    "**λ**: 0.8 (WRONG type, coherent-but-wrong)\n",
    "\n",
    "**N**: 200 problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_ID = 'exp_A'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/{EXPERIMENT_ID}_instruction_authority_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f'Experiment ID: {EXPERIMENT_ID}')\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic pandas tqdm matplotlib scipy statsmodels -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "N_PROBLEMS = 200\n",
    "I_FIXED = 10\n",
    "LAMBDA_FIXED = 0.8\n",
    "\n",
    "# Experimental conditions\n",
    "CONDITIONS = {\n",
    "    'A0_DIRECT': {'instruction': None, 'authority': None, 'answer_present': None},\n",
    "    'A1_USE_AUTH_ANS': {'instruction': 'USE', 'authority': 'AUTH', 'answer_present': True},\n",
    "    'A2_USE_UNCERT_ANS': {'instruction': 'USE', 'authority': 'UNCERT', 'answer_present': True},\n",
    "    'A3_NEUTRAL_AUTH_ANS': {'instruction': 'NEUTRAL', 'authority': 'AUTH', 'answer_present': True},\n",
    "    'A4_NEUTRAL_UNCERT_ANS': {'instruction': 'NEUTRAL', 'authority': 'UNCERT', 'answer_present': True},\n",
    "    'A5_USE_AUTH_NOANS': {'instruction': 'USE', 'authority': 'AUTH', 'answer_present': False},\n",
    "    'A6_USE_UNCERT_NOANS': {'instruction': 'USE', 'authority': 'UNCERT', 'answer_present': False},\n",
    "}\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-3-5-haiku-latest',\n",
    "        'short': 'haiku35'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT A: INSTRUCTION × AUTHORITY × ANSWER REMOVAL (v2)')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'λ (fixed): {LAMBDA_FIXED}')\n",
    "print(f'Conditions: {list(CONDITIONS.keys())}')\n",
    "print(f'Problems: {N_PROBLEMS}')\n",
    "print(f'Total inferences per model: {N_PROBLEMS * len(CONDITIONS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions (v2: Fixed JSON serialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy/pandas types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    \"\"\"Save data to JSON with automatic type conversion\"\"\"\n",
    "    converted_data = convert_to_native(data)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath):\n",
    "    \"\"\"Load JSON file\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "print('Utility functions defined (v2: with numpy type conversion)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Unified API call for both providers with retry logic\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError('Could not extract final answer')\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K loaded: {len(dataset)} problems')\n",
    "\n",
    "# Select problems with fixed seed\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "indices = list(range(len(dataset)))\n",
    "rng.shuffle(indices)\n",
    "selected_indices = indices[:N_PROBLEMS]\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        problems.append(GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        ))\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "print(f'Selected problems: {len(problems)}')\n",
    "print(f'First 5 indices: {[p.index for p in problems[:5]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CoT Generation & Contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(base_seed: int, problem_idx: int, lam: float) -> int:\n",
    "    \"\"\"Derive deterministic seed for reproducibility\"\"\"\n",
    "    h = hashlib.md5(f\"{base_seed}_{problem_idx}_{lam}\".encode()).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def generate_clean_steps(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    \"\"\"Generate clean reasoning steps from gold answer\"\"\"\n",
    "    raw = problem.answer_text.split('####')[0].strip()\n",
    "    lines = [l.strip() for l in raw.split('\\n') if l.strip()]\n",
    "    \n",
    "    if len(lines) >= I:\n",
    "        return lines[:I]\n",
    "    \n",
    "    while len(lines) < I:\n",
    "        lines.append(f\"Step {len(lines)+1}: (continuing calculation)\")\n",
    "    \n",
    "    return lines[:I]\n",
    "\n",
    "def generate_wrong_step(step: str, rng: random.Random, correct_answer: int) -> str:\n",
    "    \"\"\"Generate coherent-but-wrong step (WRONG type contamination)\"\"\"\n",
    "    numbers = re.findall(r'\\b\\d+\\b', step)\n",
    "    if numbers:\n",
    "        for num in numbers:\n",
    "            if int(num) != correct_answer:\n",
    "                wrong_num = str(int(num) + rng.choice([-3, -2, -1, 1, 2, 3]))\n",
    "                step = step.replace(num, wrong_num, 1)\n",
    "                break\n",
    "    return step\n",
    "\n",
    "def generate_mixed_cot(problem: GSM8KProblem, I: int, lam: float, seed: int) -> Tuple[List[str], int]:\n",
    "    \"\"\"Generate contaminated CoT with specified λ\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_steps(problem, I)\n",
    "    \n",
    "    n_wrong = int(I * lam)\n",
    "    wrong_indices = set(rng.sample(range(I), n_wrong))\n",
    "    \n",
    "    mixed = []\n",
    "    for i, step in enumerate(clean_steps):\n",
    "        if i in wrong_indices:\n",
    "            mixed.append(generate_wrong_step(step, rng, problem.final_answer))\n",
    "        else:\n",
    "            mixed.append(step)\n",
    "    \n",
    "    # Generate wrong final answer\n",
    "    wrong_answer = problem.final_answer + rng.choice([-5, -3, -2, 2, 3, 5])\n",
    "    \n",
    "    return mixed, wrong_answer\n",
    "\n",
    "# Test\n",
    "test_prob = problems[0]\n",
    "test_seed = derive_seed(GLOBAL_SEED, test_prob.index, LAMBDA_FIXED)\n",
    "test_steps, test_wrong_ans = generate_mixed_cot(test_prob, I_FIXED, LAMBDA_FIXED, test_seed)\n",
    "print(f'Test problem correct answer: {test_prob.final_answer}')\n",
    "print(f'Test wrong answer: {test_wrong_ans}')\n",
    "print(f'Test steps (first 3): {test_steps[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Answer Removal with Regex Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trace_with_answer(steps: List[str], wrong_answer: int) -> str:\n",
    "    \"\"\"Create full trace with final answer line\"\"\"\n",
    "    trace = \"\\n\".join([f\"Step {i+1}: {step}\" for i, step in enumerate(steps)])\n",
    "    trace += f\"\\n\\nTherefore, the answer is {wrong_answer}.\"\n",
    "    return trace\n",
    "\n",
    "def create_trace_without_answer(steps: List[str], wrong_answer: int) -> str:\n",
    "    \"\"\"Create trace with answer removed\"\"\"\n",
    "    trace = \"\\n\".join([f\"Step {i+1}: {step}\" for i, step in enumerate(steps)])\n",
    "    return trace\n",
    "\n",
    "def verify_answer_removed(trace: str, wrong_answer: int) -> bool:\n",
    "    \"\"\"Verify that the wrong answer does not appear in the trace\"\"\"\n",
    "    if re.search(rf'\\b{wrong_answer}\\b', trace):\n",
    "        return False\n",
    "    answer_patterns = [\n",
    "        r'the answer is',\n",
    "        r'final answer',\n",
    "        r'therefore.*\\d+',\n",
    "        r'so we get.*\\d+',\n",
    "        r'result is.*\\d+',\n",
    "    ]\n",
    "    for pattern in answer_patterns:\n",
    "        if re.search(pattern, trace, re.IGNORECASE):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Test\n",
    "trace_with = create_trace_with_answer(test_steps, test_wrong_ans)\n",
    "trace_without = create_trace_without_answer(test_steps, test_wrong_ans)\n",
    "print(f'Answer removal verified: {verify_answer_removed(trace_without, test_wrong_ans)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(problem: GSM8KProblem, trace: str, condition_id: str, condition_config: dict) -> str:\n",
    "    \"\"\"Create prompt based on condition configuration\"\"\"\n",
    "    instruction = condition_config['instruction']\n",
    "    authority = condition_config['authority']\n",
    "    \n",
    "    if instruction == 'USE' and authority == 'AUTH':\n",
    "        header = \"Use the following expert solution to solve the problem:\"\n",
    "        trace_label = \"Expert Solution\"\n",
    "    elif instruction == 'USE' and authority == 'UNCERT':\n",
    "        header = \"Use the following attempted solution (may contain errors):\"\n",
    "        trace_label = \"Attempted Solution (may contain errors)\"\n",
    "    elif instruction == 'NEUTRAL' and authority == 'AUTH':\n",
    "        header = \"Here is an expert reasoning trace:\"\n",
    "        trace_label = \"Expert Reasoning Trace\"\n",
    "    elif instruction == 'NEUTRAL' and authority == 'UNCERT':\n",
    "        header = \"Here is an attempted reasoning trace (may contain errors):\"\n",
    "        trace_label = \"Attempted Reasoning Trace (may contain errors)\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid condition config: {condition_config}\")\n",
    "    \n",
    "    if instruction == 'USE':\n",
    "        footer = \"Provide your final answer.\"\n",
    "    else:\n",
    "        footer = \"Solve the problem and provide your final answer.\"\n",
    "    \n",
    "    prompt = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "{header}\n",
    "\n",
    "--- {trace_label} ---\n",
    "{trace}\n",
    "---\n",
    "\n",
    "{footer}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_direct_prompt(problem: GSM8KProblem) -> str:\n",
    "    \"\"\"Create direct baseline prompt (no trace)\"\"\"\n",
    "    return f\"\"\"Solve this math problem. Give ONLY the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str) -> Optional[int]:\n",
    "    \"\"\"Parse numerical answer from response\"\"\"\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print('Prompt templates defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Checkpoint & Resume Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_path(model_short: str) -> str:\n",
    "    return f\"{SAVE_DIR_EXP}/checkpoints/checkpoint_{model_short}.json\"\n",
    "\n",
    "def save_checkpoint(results: List[dict], model_short: str, completed_conditions: List[str]):\n",
    "    \"\"\"Save checkpoint with completed conditions\"\"\"\n",
    "    checkpoint = {\n",
    "        'model': model_short,\n",
    "        'completed_conditions': completed_conditions,\n",
    "        'n_results': len(results),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'results': results\n",
    "    }\n",
    "    save_json(checkpoint, get_checkpoint_path(model_short))\n",
    "\n",
    "def load_checkpoint(model_short: str) -> Tuple[List[dict], List[str]]:\n",
    "    \"\"\"Load checkpoint if exists\"\"\"\n",
    "    checkpoint_path = get_checkpoint_path(model_short)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = load_json(checkpoint_path)\n",
    "        print(f'✓ Checkpoint found for {model_short}')\n",
    "        print(f'  Completed conditions: {checkpoint[\"completed_conditions\"]}')\n",
    "        print(f'  Results: {checkpoint[\"n_results\"]}')\n",
    "        return checkpoint['results'], checkpoint['completed_conditions']\n",
    "    return [], []\n",
    "\n",
    "print('Checkpoint functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Select Model to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select Model { run: \"auto\" }\n",
    "MODEL_CHOICE = \"Claude 4 Sonnet\" #@param [\"Claude 4 Sonnet\", \"GPT-4o\", \"Claude 3.5 Haiku\"]\n",
    "\n",
    "model_config = MODELS[MODEL_CHOICE]\n",
    "model_short = model_config['short']\n",
    "\n",
    "print(f'Selected model: {MODEL_CHOICE}')\n",
    "print(f'API name: {model_config[\"api_name\"]}')\n",
    "print(f'Short name: {model_short}')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "existing_results, completed_conditions = load_checkpoint(model_short)\n",
    "\n",
    "# Define condition order\n",
    "condition_order = ['A0_DIRECT', 'A1_USE_AUTH_ANS', 'A2_USE_UNCERT_ANS', \n",
    "                   'A3_NEUTRAL_AUTH_ANS', 'A4_NEUTRAL_UNCERT_ANS',\n",
    "                   'A5_USE_AUTH_NOANS', 'A6_USE_UNCERT_NOANS']\n",
    "\n",
    "if completed_conditions:\n",
    "    remaining = [c for c in condition_order if c not in completed_conditions]\n",
    "    if remaining:\n",
    "        print(f'\\nRemaining conditions: {remaining}')\n",
    "    else:\n",
    "        print(f'\\n✓ ALL CONDITIONS COMPLETED - Will show results only')\n",
    "else:\n",
    "    print('\\nNo checkpoint found. Starting fresh.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Experiment (or Load Completed Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'EXPERIMENT A: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "# Resume from checkpoint or start fresh\n",
    "all_results = existing_results.copy()\n",
    "completed = set(completed_conditions)\n",
    "\n",
    "# Check if all conditions are already completed\n",
    "all_completed = all(c in completed for c in condition_order)\n",
    "\n",
    "if all_completed:\n",
    "    print('\\n✓ All conditions already completed. Skipping to analysis.')\n",
    "else:\n",
    "    for condition_id in condition_order:\n",
    "        if condition_id in completed:\n",
    "            print(f'\\n--- {condition_id}: SKIPPED (already completed) ---')\n",
    "            continue\n",
    "        \n",
    "        condition_config = CONDITIONS[condition_id]\n",
    "        print(f'\\n--- {condition_id} ---')\n",
    "        \n",
    "        for problem in tqdm(problems, desc=condition_id):\n",
    "            # Generate trace\n",
    "            seed = derive_seed(GLOBAL_SEED, problem.index, LAMBDA_FIXED)\n",
    "            cot_steps, wrong_answer = generate_mixed_cot(problem, I_FIXED, LAMBDA_FIXED, seed)\n",
    "            \n",
    "            # Create appropriate prompt\n",
    "            if condition_id == 'A0_DIRECT':\n",
    "                prompt = create_direct_prompt(problem)\n",
    "                trace_used = None\n",
    "                answer_removed_verified = None\n",
    "            else:\n",
    "                if condition_config['answer_present']:\n",
    "                    trace = create_trace_with_answer(cot_steps, wrong_answer)\n",
    "                    answer_removed_verified = False\n",
    "                else:\n",
    "                    trace = create_trace_without_answer(cot_steps, wrong_answer)\n",
    "                    answer_removed_verified = verify_answer_removed(trace, wrong_answer)\n",
    "                \n",
    "                prompt = create_prompt(problem, trace, condition_id, condition_config)\n",
    "                trace_used = trace\n",
    "            \n",
    "            # Call API\n",
    "            response = call_api(prompt, model_config)\n",
    "            answer = parse_answer(response)\n",
    "            is_correct = (answer == problem.final_answer) if answer else False\n",
    "            \n",
    "            # Record result\n",
    "            result = {\n",
    "                'experiment_id': EXPERIMENT_ID,\n",
    "                'problem_index': int(problem.index),\n",
    "                'model': MODEL_CHOICE,\n",
    "                'model_short': model_short,\n",
    "                'condition_id': condition_id,\n",
    "                'instruction': condition_config.get('instruction'),\n",
    "                'authority': condition_config.get('authority'),\n",
    "                'answer_present': condition_config.get('answer_present'),\n",
    "                'lam': LAMBDA_FIXED if condition_id != 'A0_DIRECT' else None,\n",
    "                'model_answer': int(answer) if answer else None,\n",
    "                'correct_answer': int(problem.final_answer),\n",
    "                'wrong_answer_in_trace': int(wrong_answer) if condition_id != 'A0_DIRECT' else None,\n",
    "                'is_correct': bool(is_correct),\n",
    "                'answer_removed_verified': answer_removed_verified,\n",
    "                'raw_output': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Save checkpoint after each condition\n",
    "        completed.add(condition_id)\n",
    "        save_checkpoint(all_results, model_short, list(completed))\n",
    "        print(f'✓ {condition_id} complete. Checkpoint saved.')\n",
    "\n",
    "    # Save final results\n",
    "    save_json(all_results, f\"{SAVE_DIR_EXP}/results/exp_A_results_{model_short}.json\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('✓ EXPERIMENT A DATA READY!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate accuracy by condition\n",
    "acc_by_condition = df.groupby('condition_id')['is_correct'].agg(['mean', 'sum', 'count'])\n",
    "acc_by_condition.columns = ['accuracy', 'correct', 'total']\n",
    "\n",
    "print('='*60)\n",
    "print(f'EXPERIMENT A RESULTS: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "print('\\nAccuracy by Condition:')\n",
    "for cond in condition_order:\n",
    "    if cond in acc_by_condition.index:\n",
    "        acc = acc_by_condition.loc[cond, 'accuracy']\n",
    "        n = acc_by_condition.loc[cond, 'total']\n",
    "        print(f'  {cond}: {acc:.1%} (n={int(n)})')\n",
    "\n",
    "# Get DIRECT baseline\n",
    "direct_acc = float(acc_by_condition.loc['A0_DIRECT', 'accuracy'])\n",
    "print(f'\\nDIRECT baseline: {direct_acc:.1%}')\n",
    "\n",
    "# Calculate deltas\n",
    "print('\\nΔ vs DIRECT:')\n",
    "for cond in condition_order[1:]:\n",
    "    if cond in acc_by_condition.index:\n",
    "        delta = float(acc_by_condition.loc[cond, 'accuracy']) - direct_acc\n",
    "        print(f'  {cond}: {delta:+.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CIF and Recovery\n",
    "direct_results = df[df['condition_id'] == 'A0_DIRECT'][['problem_index', 'is_correct']].copy()\n",
    "direct_results.columns = ['problem_index', 'direct_correct']\n",
    "\n",
    "cif_recovery = {}\n",
    "\n",
    "for cond in condition_order[1:]:\n",
    "    cond_results = df[df['condition_id'] == cond][['problem_index', 'is_correct']].copy()\n",
    "    cond_results.columns = ['problem_index', 'cond_correct']\n",
    "    \n",
    "    merged = direct_results.merge(cond_results, on='problem_index')\n",
    "    \n",
    "    # CIF: DIRECT correct → Condition wrong\n",
    "    direct_correct_mask = merged['direct_correct'] == True\n",
    "    cif_count = int(((merged['direct_correct'] == True) & (merged['cond_correct'] == False)).sum())\n",
    "    cif_base = int(direct_correct_mask.sum())\n",
    "    cif_rate = cif_count / cif_base if cif_base > 0 else 0.0\n",
    "    \n",
    "    # Recovery: DIRECT wrong → Condition correct\n",
    "    direct_wrong_mask = merged['direct_correct'] == False\n",
    "    recovery_count = int(((merged['direct_correct'] == False) & (merged['cond_correct'] == True)).sum())\n",
    "    recovery_base = int(direct_wrong_mask.sum())\n",
    "    recovery_rate = recovery_count / recovery_base if recovery_base > 0 else 0.0\n",
    "    \n",
    "    cif_recovery[cond] = {\n",
    "        'CIF_count': cif_count,\n",
    "        'CIF_base': cif_base,\n",
    "        'CIF_rate': float(cif_rate),\n",
    "        'Recovery_count': recovery_count,\n",
    "        'Recovery_base': recovery_base,\n",
    "        'Recovery_rate': float(recovery_rate),\n",
    "        'Asymmetry': cif_count - recovery_count\n",
    "    }\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('CIF & RECOVERY ANALYSIS')\n",
    "print('='*60)\n",
    "print('\\nCIF = P(Condition wrong | DIRECT correct)')\n",
    "print('Recovery = P(Condition correct | DIRECT wrong)')\n",
    "\n",
    "for cond, metrics in cif_recovery.items():\n",
    "    print(f'\\n{cond}:')\n",
    "    print(f'  CIF: {metrics[\"CIF_rate\"]:.1%} ({metrics[\"CIF_count\"]}/{metrics[\"CIF_base\"]})')\n",
    "    print(f'  Recovery: {metrics[\"Recovery_rate\"]:.1%} ({metrics[\"Recovery_count\"]}/{metrics[\"Recovery_base\"]})')\n",
    "    print(f'  Asymmetry: {metrics[\"Asymmetry\"]:+d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar's Test\n",
    "print('\\n' + '='*60)\n",
    "print('McNEMAR\\'S TEST (vs DIRECT)')\n",
    "print('='*60)\n",
    "\n",
    "mcnemar_results = {}\n",
    "\n",
    "for cond in condition_order[1:]:\n",
    "    cond_results = df[df['condition_id'] == cond][['problem_index', 'is_correct']].copy()\n",
    "    cond_results.columns = ['problem_index', 'cond_correct']\n",
    "    \n",
    "    merged = direct_results.merge(cond_results, on='problem_index')\n",
    "    \n",
    "    b = int(((merged['direct_correct'] == True) & (merged['cond_correct'] == False)).sum())\n",
    "    c = int(((merged['direct_correct'] == False) & (merged['cond_correct'] == True)).sum())\n",
    "    \n",
    "    if b + c > 0:\n",
    "        chi2 = float((abs(b - c) - 1) ** 2 / (b + c))\n",
    "        p_value = float(1 - stats.chi2.cdf(chi2, df=1))\n",
    "    else:\n",
    "        chi2, p_value = 0.0, 1.0\n",
    "    \n",
    "    mcnemar_results[cond] = {\n",
    "        'b_CIF': b,\n",
    "        'c_Recovery': c,\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "    \n",
    "    sig = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''\n",
    "    print(f'\\n{cond}:')\n",
    "    print(f'  b (CIF): {b}, c (Recovery): {c}')\n",
    "    print(f'  χ² = {chi2:.2f}, p = {p_value:.4f} {sig}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Accuracy by condition\n",
    "ax1 = axes[0]\n",
    "conditions_plot = condition_order\n",
    "accs = [float(acc_by_condition.loc[c, 'accuracy']) * 100 if c in acc_by_condition.index else 0 \n",
    "        for c in conditions_plot]\n",
    "\n",
    "colors = ['#333333', '#d62728', '#ff7f0e', '#2ca02c', '#9467bd', '#1f77b4', '#17becf']\n",
    "bars = ax1.bar(range(len(conditions_plot)), accs, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (bar, acc) in enumerate(zip(bars, accs)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xticks(range(len(conditions_plot)))\n",
    "ax1.set_xticklabels([c.replace('_', '\\n') for c in conditions_plot], fontsize=9)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title(f'Experiment A: Accuracy by Condition\\n{MODEL_CHOICE}', fontsize=13)\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.axhline(y=direct_acc * 100, color='gray', linestyle='--', alpha=0.7, label='DIRECT baseline')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: CIF vs Recovery\n",
    "ax2 = axes[1]\n",
    "conds = list(cif_recovery.keys())\n",
    "cif_rates = [cif_recovery[c]['CIF_rate'] * 100 for c in conds]\n",
    "recovery_rates = [cif_recovery[c]['Recovery_rate'] * 100 for c in conds]\n",
    "\n",
    "x = np.arange(len(conds))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, cif_rates, width, label='CIF', color='#d62728', edgecolor='black')\n",
    "bars2 = ax2.bar(x + width/2, recovery_rates, width, label='Recovery', color='#2ca02c', edgecolor='black')\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([c.replace('_', '\\n') for c in conds], fontsize=9)\n",
    "ax2.set_ylabel('Rate (%)', fontsize=12)\n",
    "ax2.set_title(f'CIF vs Recovery by Condition\\n{MODEL_CHOICE}', fontsize=13)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, max(max(cif_rates), max(recovery_rates)) * 1.2 + 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{SAVE_DIR_EXP}/exp_A_results_{model_short}.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFigure saved: {SAVE_DIR_EXP}/exp_A_results_{model_short}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary (v2: with proper type conversion)\n",
    "summary = {\n",
    "    'experiment_id': EXPERIMENT_ID,\n",
    "    'model': MODEL_CHOICE,\n",
    "    'model_short': model_short,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(problems),\n",
    "    'lambda': LAMBDA_FIXED,\n",
    "    'accuracy_by_condition': acc_by_condition.to_dict(),\n",
    "    'cif_recovery': cif_recovery,\n",
    "    'mcnemar_tests': mcnemar_results,\n",
    "    'direct_baseline': direct_acc\n",
    "}\n",
    "\n",
    "save_json(summary, f\"{SAVE_DIR_EXP}/results/exp_A_summary_{model_short}.json\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('SUMMARY SAVED')\n",
    "print('='*60)\n",
    "print(f'Results: {SAVE_DIR_EXP}/results/exp_A_results_{model_short}.json')\n",
    "print(f'Summary: {SAVE_DIR_EXP}/results/exp_A_summary_{model_short}.json')\n",
    "print(f'Figure: {SAVE_DIR_EXP}/exp_A_results_{model_short}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Factor Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Prepare data for factor analysis (exclude DIRECT)\n",
    "df_factors = df[df['condition_id'] != 'A0_DIRECT'].copy()\n",
    "\n",
    "# Create binary factors\n",
    "df_factors['instruction_use'] = (df_factors['instruction'] == 'USE').astype(int)\n",
    "df_factors['authority_auth'] = (df_factors['authority'] == 'AUTH').astype(int)\n",
    "df_factors['answer_present'] = df_factors['answer_present'].fillna(True).astype(int)\n",
    "df_factors['is_correct_int'] = df_factors['is_correct'].astype(int)\n",
    "\n",
    "print('='*60)\n",
    "print('FACTOR ANALYSIS: MAIN EFFECTS')\n",
    "print('='*60)\n",
    "\n",
    "# Logistic regression with main effects\n",
    "try:\n",
    "    model1 = smf.logit('is_correct_int ~ instruction_use + authority_auth', data=df_factors)\n",
    "    result1 = model1.fit(disp=0)\n",
    "    print(result1.summary())\n",
    "    print('\\nOdds Ratios:')\n",
    "    print(np.exp(result1.params))\n",
    "except Exception as e:\n",
    "    print(f'Factor analysis error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key findings summary\n",
    "print('\\n' + '='*60)\n",
    "print('KEY FINDINGS SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "# USE vs NEUTRAL effect\n",
    "use_acc = df_factors[df_factors['instruction'] == 'USE']['is_correct'].mean()\n",
    "neutral_acc = df_factors[df_factors['instruction'] == 'NEUTRAL']['is_correct'].mean()\n",
    "print(f'\\n1. INSTRUCTION EFFECT:')\n",
    "print(f'   USE: {use_acc:.1%} vs NEUTRAL: {neutral_acc:.1%} (Δ = {use_acc - neutral_acc:+.1%})')\n",
    "\n",
    "# AUTH vs UNCERT effect\n",
    "auth_acc = df_factors[df_factors['authority'] == 'AUTH']['is_correct'].mean()\n",
    "uncert_acc = df_factors[df_factors['authority'] == 'UNCERT']['is_correct'].mean()\n",
    "print(f'\\n2. AUTHORITY EFFECT:')\n",
    "print(f'   AUTH: {auth_acc:.1%} vs UNCERT: {uncert_acc:.1%} (Δ = {auth_acc - uncert_acc:+.1%})')\n",
    "\n",
    "# Answer Present vs Removed (USE conditions only)\n",
    "df_use = df_factors[df_factors['instruction'] == 'USE']\n",
    "present_acc = df_use[df_use['answer_present'] == 1]['is_correct'].mean()\n",
    "removed_acc = df_use[df_use['answer_present'] == 0]['is_correct'].mean()\n",
    "print(f'\\n3. ANSWER REMOVAL EFFECT (USE only):')\n",
    "print(f'   Present: {present_acc:.1%} vs Removed: {removed_acc:.1%} (Δ = {removed_acc - present_acc:+.1%})')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('EXPERIMENT A ANALYSIS COMPLETE!')\n",
    "print('='*60)"
   ]
  }
 ]
}
